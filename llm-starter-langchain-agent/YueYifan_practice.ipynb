{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ebed44-a2d2-4965-9048-7d4caece6d82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. å‡†å¤‡ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bdcf5-f21e-40f2-8979-548cb1db6ba5",
   "metadata": {},
   "source": [
    "### 1.1 å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66b4b-15f8-47fa-a6c0-70795799ad31",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®‰è£…ä¸€äº›é¢å¤–çš„åº“ï¼Œä¾‹å¦‚ langchain å’Œ python-dotenvã€‚\n",
    "\n",
    "å‰è€…ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ„å»ºåŸºäºLLMçš„åº”ç”¨ç¨‹åºçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œè€Œåè€…åœ¨ä¸ºåœ¨çº¿LLMæœåŠ¡è®¾ç½®APIå¯†é’¥æ–¹é¢ä¸ºæˆ‘ä»¬èŠ‚çœäº†æ—¶é—´ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä¸‹ä¸€èŠ‚ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0608b18-2faf-40f8-beaf-efbd713e4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langchain==0.0.338\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ce/3f/1dafc52526337d1c554227b0e6f16a1aee18e63bf5cd03fd7774297059b2/langchain-0.0.338-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jsonpatch<2.0,>=1.33\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (2.31.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (4.0.2)\n",
      "Collecting langsmith<0.1.0,>=0.0.63\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/84/9e/208314830d8c523dae4dec41ab5aeeb2d42dc1667bbc3ff8b875244b3012/langsmith-0.0.66-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (1.25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (1.10.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (3.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain==0.0.338) (5.4.1)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/1c/0b66318368b1c9ef51c5c8560530b8ef842164e10eea08cacb06739388e0/SQLAlchemy-2.0.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: anyio<4.0 in /home/drosrin/.local/lib/python3.10/site-packages (from langchain==0.0.338) (3.7.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/e2/528c52001a743a7faa28e6d3095d9f01b472d3efee62d62101403bf1a70a/dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/drosrin/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (6.0.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<4.0->langchain==0.0.338) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/drosrin/.local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.338) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/drosrin/.local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.338) (1.1.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/drosrin/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.338) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/drosrin/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.338) (4.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.338) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.338) (2020.6.20)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/da/ab/7cc6502628565d70dce2edb619d87554d65ac4e2f17c805a5a45bfaefa5c/greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m613.2/613.2 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/drosrin/.local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (23.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2a/e2/5d3f6ada4297caebe1a2add3b126fe800c96f56dbe5d1988a2cbe0b267aa/mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, mypy-extensions, marshmallow, jsonpatch, greenlet, typing-inspect, SQLAlchemy, langsmith, dataclasses-json, langchain\n",
      "Successfully installed SQLAlchemy-2.0.23 dataclasses-json-0.6.2 greenlet-3.0.1 jsonpatch-1.33 langchain-0.0.338 langsmith-0.0.66 marshmallow-3.20.1 mypy-extensions-1.0.0 tenacity-8.2.3 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install langchain, the library we will learn during our courses\n",
    "!pip install langchain==0.0.338 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892e98f2-bbf1-4c8f-a755-5a8c64e9a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/44/2f/62ea1c8b593f4e093cc1a7768f0d46112107e790c3e478532329e434f00b/python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install dotenv, auto-load environment variables from `.env` files\n",
    "!pip install python-dotenv==1.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919c726-32d3-4025-9e8f-db00e5ca1c74",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œè®©æˆ‘ä»¬å®‰è£…ç”¨äºå¯¹å†…å®¹è¿›è¡Œæ ‡è®°åŒ–å’Œå­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸Šçš„åº“ï¼Œå³ tiktoken å’Œ faiss-cpuã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3e332c-0324-42ca-b5ca-8be73cfea0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken==0.5.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m773.9/773.9 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/drosrin/.local/lib/python3.10/site-packages (from tiktoken==0.5.1) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/drosrin/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.10.3 tiktoken-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken, the library used by OpenAI models for tokenizing text strings\n",
    "!pip install tiktoken==0.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55858b86-2ff1-46a7-a473-78295437ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting faiss-cpu==1.7.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/a6/4caf215afd86e3b365f3ba0d9c01800d46bc8e42e65fe3667dd9dd3a3213/faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# Install faiss-cpu, a vector database for storing content along with embedding vectors\n",
    "!pip install faiss-cpu==1.7.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2865033a-0683-4b6a-a478-8f6b9d120765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting wikipedia==1.4.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/drosrin/.local/lib/python3.10/site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/drosrin/.local/lib/python3.10/site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/drosrin/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/drosrin/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=ba5895939636f49625732dc1b42f44ac3e34f124af5b4bcc16102fc7ba6b0346\n",
      "  Stored in directory: /home/drosrin/.cache/pip/wheels/65/c7/be/e5fdfc41da826908cd24dd6daa2fe2e3e7724c83d0fcc85884\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f5f4-63d3-441d-8c9a-f7976f33555c",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œå®‰è£…ä¸€äº›ç”¨äºè®¿é—®å¤–éƒ¨æœåŠ¡çš„åº“ï¼Œä¾‹å¦‚ wikipediaã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a14613b-82a4-40b2-8c82-770d5cb7b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: wikipedia==1.4.0 in /home/drosrin/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/drosrin/.local/lib/python3.10/site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/drosrin/.local/lib/python3.10/site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/drosrin/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/drosrin/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d50-d9ac-44b3-b7c6-25b084474119",
   "metadata": {},
   "source": [
    "æœ€åï¼Œä¸ºäº†æµ‹è¯•å®‰è£…å’ŒAPIå¯†é’¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿˜å®‰è£…ç›¸åº”ä¾›åº”å•†çš„SDKåº“ï¼ˆå³OpenAIå’Œæ™ºè°±AIï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2623724c-492a-4737-be44-0ae014214042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openai==1.3.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/95/22a9a81cebd54e18841da429f05f06ed867648768f7af938ad34f13197fd/openai-1.3.3-py3-none-any.whl (220 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.3/220.3 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx<1,>=0.23.0 in /home/drosrin/.local/lib/python3.10/site-packages (from openai==1.3.3) (0.24.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.3) (1.7.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/drosrin/.local/lib/python3.10/site-packages (from openai==1.3.3) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/drosrin/.local/lib/python3.10/site-packages (from openai==1.3.3) (4.7.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /home/drosrin/.local/lib/python3.10/site-packages (from openai==1.3.3) (3.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/drosrin/.local/lib/python3.10/site-packages (from openai==1.3.3) (1.10.9)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/drosrin/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/drosrin/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<4,>=3.5.0->openai==1.3.3) (3.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai==1.3.3) (2020.6.20)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/drosrin/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (0.17.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/drosrin/.local/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai==1.3.3) (0.14.0)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# Install openai, official SDK by OpenAI for invoking GPT models\n",
    "!pip install openai==1.3.3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1338b17d-d953-4742-9fed-e8a2080170b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting zhipuai==1.0.7\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/b5/cb50cd478426b17503154585be799e09506b70944e2fe2402aa029987fa2/zhipuai-1.0.7-py3-none-any.whl (7.9 kB)\n",
      "Collecting dataclasses\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /home/drosrin/.local/lib/python3.10/site-packages (from zhipuai==1.0.7) (2.31.0)\n",
      "Collecting cachetools\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from zhipuai==1.0.7) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/drosrin/.local/lib/python3.10/site-packages (from requests->zhipuai==1.0.7) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->zhipuai==1.0.7) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->zhipuai==1.0.7) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->zhipuai==1.0.7) (1.26.5)\n",
      "Installing collected packages: dataclasses, cachetools, zhipuai\n",
      "Successfully installed cachetools-5.3.2 dataclasses-0.6 zhipuai-1.0.7\n"
     ]
    }
   ],
   "source": [
    "# Install zhipu, official SDK by OpenAI for invoking ChatGLM models\n",
    "!pip install zhipuai==1.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972013-340f-427f-9038-12cde927c9a7",
   "metadata": {},
   "source": [
    "### 1.2 ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6cfdeb-fdbf-4a09-a8fa-0ed98e470606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZHIPUAI_API_KEY']='c075c10b0d98a821e49af2ba4b5b1424.xk4AlAwO9oUiOEyw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017855e-c925-4264-9319-e47ebcbe250b",
   "metadata": {},
   "source": [
    "### 1.3 æµ‹è¯•å‡†å¤‡æ˜¯å¦æˆåŠŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af19cdeb-20c5-44aa-85b2-6df9422bf5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" æˆ‘çš„åå­—æ˜¯ç®€ã€‚ä½ çš„åå­—å«ä»€ä¹ˆï¼Ÿ\"\n"
     ]
    }
   ],
   "source": [
    "# Test zhipuai installation\n",
    "import os\n",
    "import zhipuai\n",
    "\n",
    "zhipuai.api_key = os.getenv('ZHIPUAI_API_KEY')  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "completion = zhipuai.model_api.invoke(\n",
    "    model='chatglm_turbo',\n",
    "    prompt=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0.,\n",
    ")\n",
    "\n",
    "print(completion['data']['choices'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0678d-8625-41ce-b335-cb09274c868d",
   "metadata": {},
   "source": [
    "## 2. LangchainåŸºç¡€ç»ƒä¹ ï¼ˆåŸºäºæ™ºè°±LLMï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07742-bfc3-4e59-888f-f5619a9fe247",
   "metadata": {},
   "source": [
    "ä¸OpenAIä¸åŒï¼ŒLangChainå¹¶ä¸åŸç”Ÿæ”¯æŒæ™ºè°±AIçš„åœ¨çº¿LLMæœåŠ¡ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªåŒ…è£…ç±»æ¥å°†æ™ºè°±AIçš„ChatGLPæœåŠ¡ç§»æ¤åˆ°LangChainï¼Œè¿™è¦å½’åŠŸäºLangChainçš„æ¨¡å—åŒ–æ¥å£ã€‚è¿™åº”è¯¥ç±»ä¼¼äºæˆ‘ä»¬ä½¿ç”¨OpenAIçš„GPTæœåŠ¡æ—¶çš„æ„Ÿè§‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c752ae-14ff-451b-a30d-cb1190340da2",
   "metadata": {},
   "source": [
    "### 2.1 æ£€æŸ¥ZhipuAI wrapperæ˜¯å¦å­˜åœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c40040-163c-4df3-a869-15a4fa931043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 drosrin drosrin   3684 11æœˆ 24 19:01 zhipuai_embedding.py\n",
      "-rw-r--r-- 1 drosrin drosrin   6160 11æœˆ 21 02:05 zhipuai_llm.py\n"
     ]
    }
   ],
   "source": [
    "# Check ZhipuAI wrapper existence\n",
    "!ls -la | grep \"zhipuai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e2d6-86c1-4900-8398-fa25a3559aea",
   "metadata": {},
   "source": [
    "### 2.2 ç®€å•ä½¿ç”¨ä¾‹å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18bd3d2f-b446-42a6-9920-7ad218dfc003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" æˆ‘çš„åå­—æ˜¯ç®€ã€‚ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88ab0-0ba3-4140-a939-ae91f39b4521",
   "metadata": {},
   "source": [
    "#### ç»ƒä¹ 1 - \"è®¡ç®—æ—¶é—´å¤æ‚åº¦\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc11a5-6e42-4419-b3f6-ad8d0f2fa105",
   "metadata": {},
   "source": [
    "> ğŸ’ª Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> \n",
    "> ```\n",
    "> You will be provided with Python code, and your task is to calculate its time complexity.\n",
    ">\n",
    "> def foo(n, k):\n",
    ">    accum = 0\n",
    ">    for i in range(n):\n",
    ">        for l in range(k):\n",
    ">            accum += i\n",
    ">    return accum\n",
    "> ```\n",
    "> \n",
    "> ---------------------------\n",
    "> Try to change the Python code for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d167350b-bca4-4bd0-9a15-0f10e8d6187d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" To calculate the time complexity of this code, we need to analyze the number of operations inside the function.\\n\\n1. There is one outer loop iterating over `n` elements, and inside it, there is another loop iterating over `k` elements.\\n2. In each iteration of the inner loop, we perform one addition operation (accum += i).\\n\\nSo, the total number of additions is `n * k`.\\n\\nNow, let's look at the constants and variables in the problem:\\n\\n- `n` and `k` are the input parameters, and they are assumed to be positive integers.\\n- The addition operation has a constant time complexity of 1.\\n\\nBased on the above analysis, the time complexity of the function `foo(n, k)` is:\\n\\n`O(n * k)`\\n\\nwhere O(n * k) represents the big-O notation for the function, meaning that the function's running time grows linearly with the product of `n` and `k`.\"\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "\n",
    "prompt = \"\"\"\n",
    "You will be provided with a Python code. You task is to calculate its time Complexity is big-O notations. Let's think step by step.\n",
    "```\n",
    "def foo(n, k):\n",
    "   accum = 0\n",
    "   for i in range(n):\n",
    "       for l in range(k):\n",
    "           accum += i\n",
    "   return accum\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model = 'chatglm_turbo', temperature = 0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19a3a-dd46-464b-8612-c103dd0e3427",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ç»ƒä¹ 2 - â€œå¾®åšæƒ…æ„Ÿåˆ†æâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d45ed-cc60-4d04-a4c6-6719721e4889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> ğŸ’ª Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "> positive, neutral, or negative.\n",
    "> \n",
    "> I loved the new Batman movie!\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the tweet text for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e4b7ba-2bb0-46cc-9407-7855b101710c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Positive.\"\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "\n",
    "prompt = \"\"\"\n",
    "You will be provided with a tweet. Your task is to classify its sentiment into 'positive', 'neutral' or 'negative'.\n",
    "Firstly, you are provided with following examples:\n",
    "Example 1:\n",
    "User: This movie is not entertaining. My whole family slept in the cinema for 2 hours.\n",
    "AI: Negative.\n",
    "Example 2:\n",
    "User: This is a fantastic game! It's my Game Of The Year.\n",
    "AI: Positive.\n",
    "Example 3:\n",
    "User: This book has some advantages but also some drawbacks. In conclusion, it's just so-so.\n",
    "AI: Neutral.\n",
    "\n",
    "Now you are provided with the tweet awaits for catagorizing:\n",
    "User: I loved this new Batman movie!\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model = 'chatglm_turbo', temperature = 0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a19-cabc-4801-9178-9024748a8a8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ç»ƒä¹ 3 - â€œæœºåœºä»£å·æå–â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf980e2b-c9c0-4cdf-8b44-062027f8fa8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> ğŸ’ª Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "> \n",
    "> I want to fly from Orlando to Boston\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the city names and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a10b019-a071-407a-8e4e-0fb13cdbc2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Orlando = MCO; Boston = BOS;\"\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "prompt = \"\"\"\n",
    "You will be provided with a sentence. Your task is to extract the airport codes from it.\n",
    "Firstly you are provided with some examples:\n",
    "Example 1:\n",
    "User: Today Jane will fly to New York.\n",
    "AI: New York = EWR;\n",
    "Example 2:\n",
    "User: Attention all passengers, the plane from Washington D.C. to San Fransisco is now abroading.\n",
    "AI: Washington D.C. = DCA; San Fransisco = SFO;\n",
    "\n",
    "Now you are provided with the sentence awaits for extracting:\n",
    "User: I want to fly from Orlando to Boston.\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model = 'chatglm_turbo', temperature = 0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09091de-7a0a-4338-918c-cca0608a2397",
   "metadata": {},
   "source": [
    "### 2.3 æ¢ç´¢LLMå±€é™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4e6a1b5-e44e-47ff-9584-8d3040106eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1st response: \" The 1986 FIFA World Cup was won by Argentina, who defeated West Germany in the final match by a score of 3-2.\"\n",
      "- 2nd response: \" As an AI language model, I cannot predict future events. The 2022 FIFA World Cup will take place in Qatar, but the winner is yet to be determined as the tournament has not occurred. Keep an eye on the upcoming events and follow the updates to find out who will win the tournament.\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Which team won the 1986 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 1st response: {response}')\n",
    "\n",
    "prompt = \"\"\"Which team won the 2022 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 2nd response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1d69741-cf70-453b-abb8-b865aee1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpt: \" First, let's add 4829 and 2930:\\n\\n4829 + 2930 = 7759\\n\\nNow, let's multiply the sum by 1923:\\n\\n7759 * 1923 = 146,918,467\\n\\nSo, (4829 + 2930) * 1923 = 146,918,467.\"\n",
      "- truth:\n",
      "\n",
      " 14920557\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Sum 4829 and 2930, and then multiply by 1923.\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(f'- gpt: {response}')\n",
    "print(f'- truth:\\n\\n {(4829 + 2930) * 1923}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15a3b-39e5-4420-b81e-75a4aed444e0",
   "metadata": {},
   "source": [
    "### 2.4 æ¢ç´¢Langchainæ¨¡å—åŒ–ç»„ä»¶è®¾è®¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72490709-13f7-4fd1-9fa6-6fed0c61e876",
   "metadata": {},
   "source": [
    "ğŸ“Œ æ‰“å¼€è°ƒè¯•å’Œè¯¦ç»†æ¨¡å¼\n",
    "\n",
    "å¦‚æœæ‚¨æ˜¯åˆå­¦è€…ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åœ¨LangChainä¸­æ‰“å¼€è°ƒè¯•å’Œè¯¦ç»†æ¨¡å¼ï¼Œåœ¨LLMåº”ç”¨ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­æ˜¾ç¤ºä¸­é—´æ­¥éª¤çš„é¢å¤–ä¿¡æ¯ã€‚\n",
    "æŸ¥çœ‹æç¤ºå¦‚ä½•å¡«å……ä»¥åŠä¸­é—´LLMç”Ÿæˆçš„å“åº”æ˜¯ä¸ªå¥½ä¸»æ„ï¼ˆåœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä¸åº”æ‰“å°ä»»ä½•è¾“å‡ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6857b586-03a6-4700-a1a5-83b3e034a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee3a0-2cf6-4dd8-9a4e-f1dfb70227db",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afa2ad97-46ce-424a-afb7-a94b7ef674ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"colors\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] [3.76s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\\\" red, blue, green, yellow, purple\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\" red', 'blue', 'green', 'yellow', 'purple\"']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Custom output parser, split comma separated strings and return as list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# [2] System message template, declare task requirement as prompt\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# [3] Human message template, here we use Python format string syntax\n",
    "# (https://docs.python.org/3/library/string.html#formatstrings)\n",
    "human_template = '{text}'\n",
    "\n",
    "# [4] We send both messages to LLM for response\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# [5] Build up simple chain with LangChain Expression Language\n",
    "# (https://python.langchain.com/docs/expression_language/)\n",
    "chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()\n",
    "\n",
    "# [6] Call simple chain with human input, i.e., text = \"colors\"\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5c10-9981-474c-9ad2-288ede059f10",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40377-0d36-4250-9585-41dc0a63ea56",
   "metadata": {},
   "source": [
    "åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºä¼ ç»Ÿçš„Chainæ¥å£ã€‚é¦–å…ˆå¼€å§‹é‡å†™å‰ä¸€èŠ‚ä¸­çš„ICELé£æ ¼é“¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "652675c6-df3b-426e-b6b6-97efc878af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [3.74s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3.74s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['\" red', 'blue', 'green', 'yellow', 'purple\"']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = '{text}'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# Equivalent to `chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()`\n",
    "chain = LLMChain(\n",
    "    llm=ZhipuAILLM(model='chatglm_turbo'),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d039d6c-ad90-4b0b-bbba-1a71588878d6",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ›´å¤æ‚çš„é“¾ã€‚æˆ‘ä»¬å°†ä»‹ç»ä¸€ä¸ªç®€å•çš„ä¸¤é˜¶æ®µè¿ç»­é“¾ï¼Œå…¶ä¸­ï¼š\n",
    "\n",
    "1. ä¸ºä¸€å®¶åˆ¶é€ æŸç§äº§å“çš„å…¬å¸æå‡ºåç§°\n",
    "2. ä¸ºæå‡ºçš„å…¬å¸å†™ä¸€ä¸ªç®€çŸ­çš„æè¿°ï¼ˆå³å£å·ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca8bcd6b-f5e7-44f0-90bd-192e352c91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the best name to describe a company that makes Pure Milk?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [4.99s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\" Pure Milk Company,\\\\\\\" \\\\\\\"Pure Dairy,\\\\\\\" or \\\\\\\"Whitewave Milk Products.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for consumers.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] [4.99s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\" Pure Milk Company,\\\\\\\" \\\\\\\"Pure Dairy,\\\\\\\" or \\\\\\\"Whitewave Milk Products.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for consumers.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"company_name\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\" Pure Milk Company,\\\\\\\" \\\\\\\"Pure Dairy,\\\\\\\" or \\\\\\\"Whitewave Milk Products.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for consumers.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a 20 words slogan for the following company:\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\" Pure Milk Company,\\\\\\\" \\\\\\\"Pure Dairy,\\\\\\\" or \\\\\\\"Whitewave Milk Products.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for consumers.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] [2.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Nurturing Health, Delivering Pure Joy!\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] [2.39s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Nurturing Health, Delivering Pure Joy!\\\\\\\"\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] [7.39s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\\" \\\\\\\"Nurturing Health, Delivering Pure Joy!\\\\\\\"\\\"\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" \\\\\"Nurturing Health, Delivering Pure Joy!\\\\\"\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "product = 'Pure Milk'\n",
    "\n",
    "# [0] The same LLM instance shared by both chains (remember LLM is stateless)\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [1] Build name chain (1st chain)\n",
    "name_template = \"\"\"What is the best name to describe a company that makes {product}?\"\"\"\n",
    "name_prompt = ChatPromptTemplate.from_template(name_template)\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "# [2] Build slogan chain (2nd chain)\n",
    "slogan_template = \"\"\"Write a 20 words slogan for the following company:{company_name}\"\"\"\n",
    "slogan_prompt = ChatPromptTemplate.from_template(slogan_template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "# [3] Construct final chain in a sequencial manner\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, slogan_chain])\n",
    "\n",
    "# [4] Call our final chain to propose and write slogan\n",
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee8a2-5574-4f87-9d45-8f2161edd5b6",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dca0f-aef3-4a36-8edc-d68d55c40893",
   "metadata": {},
   "source": [
    "å›é¡¾ä¸€ä¸‹æˆ‘ä»¬è¯´è¿‡çš„LLMæœ¬è´¨ä¸Šæ˜¯æ— çŠ¶æ€çš„ï¼Œå³åç»­è°ƒç”¨æ°¸è¿œä¸ä¼šå›å¿†èµ·åœ¨ä¹‹å‰çš„è°ƒç”¨ä¸­æåˆ°çš„ä¿¡æ¯ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜è¿™ä¸ªè¯´æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06801d81-a3f2-46e1-a7db-1a33bba1b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Hello, my name is Charles.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.41s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello Charles! It's a pleasure to meet you. How can I assist you today? If you have any questions or need help with anything, feel free to ask.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Initial message: \" Hello Charles! It's a pleasure to meet you. How can I assist you today? If you have any questions or need help with anything, feel free to ask.\"\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Well, what is my name?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [4.89s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" As an AI language model, I don't have real-time awareness or the ability to know your name without you providing it. Please feel free to share your name, and I'll be happy to address you by your preferred name.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Follow-up message: \" As an AI language model, I don't have real-time awareness or the ability to know your name without you providing it. Please feel free to share your name, and I'll be happy to address you by your preferred name.\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "print(f'Initial message: {llm.predict(\"Hello, my name is Charles.\")}')\n",
    "print(f'Follow-up message: {llm.predict(\"Well, what is my name?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d238-cf65-4356-bbc6-797acef6fed1",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨LangChainä¸­ä¸ºä¸€ä¸ªå¯¹è¯åº”ç”¨ç¨‹åºæ·»åŠ ä¸€ä¸ªè®°å¿†æ¨¡å—ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ConversationBufferMemoryè®°å¿†æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1731ea5-edd8-42c3-9a3b-a4d1ccc02913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Hello, my name is Charles.\",\n",
      "  \"chat_history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\n\\n\\nNew human question: Hello, my name is Charles.\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [5.06s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\\\\\'m here to assist you in any way I can.\\\\n\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5.06s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\\\\\'m here to assist you in any way I can.\\\\n\\\"\"\n",
      "}\n",
      "Initial message: \" Hello, Charles! It\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\'m here to assist you in any way I can.\\n\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Well, what is my name?\",\n",
      "  \"chat_history\": \"Human: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\\\\\'m here to assist you in any way I can.\\\\n\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\\\\\'m here to assist you in any way I can.\\\\n\\\"\\n\\nNew human question: Well, what is my name?\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [1.94s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Charles, your name is Charles.\\\\\\\\n\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [1.94s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Charles, your name is Charles.\\\\\\\\n\\\\\\\"\\\"\"\n",
      "}\n",
      "Follow-up message: \" \\\"Charles, your name is Charles.\\\\n\\\"\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# [2] Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [3] Memory should work with Chain for effect\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b293b-93c1-4c4c-9b41-e0ecda1d0c31",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394b53-6399-4e64-9a51-a9544a62590e",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„æ£€ç´¢æ–¹å¼ï¼Œå³åŸºäºå‘é‡å­˜å‚¨çš„æ£€ç´¢å™¨ï¼Œå¹¶çœ‹çœ‹å®ƒåœ¨LangChainç»„ä»¶ä¸­çš„å·¥ä½œåŸç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "496c96be-ce08-4cb3-8b2c-6cc0d5b060a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc #0: æˆ‘ç”¨é‚£åªæ²¡å—ä¼¤çš„æ‰‹æŠ½å‡ºæ‰‹æªï¼Œéšç€è¿™ç¾¤çªç„¶ç‹‚çƒ­èµ·æ¥çš„å—ä¼¤å’Œæ²¡å—ä¼¤çš„äººï¼Œæ²¿ç€é’¢é“é€šé“ï¼Œå‘åœ°çƒé©¾é©¶å®¤å†²å»ã€‚å‡ºä¹æ„æ–™ï¼Œä¸€è·¯ä¸Šæˆ‘ä»¬å‡ ä¹æ²¡é‡åˆ°æŠµæŠ—ï¼Œå€’æ˜¯æœ‰è¶Šæ¥è¶Šå¤šçš„äººä»é”™ç»¼å¤æ‚çš„é’¢é“é€šé“çš„å„ä¸ªåˆ†æ”¯ä¸­åŠ å…¥æˆ‘ä»¬ã€‚æœ€å...\n",
      "doc #1: â€œè¿™ä¸ªå°ä¸–ç•Œæ­»äº†ã€‚å­©å­ä»¬ï¼Œè°èƒ½è¯´å‡ºä¸ºä»€ä¹ˆï¼Ÿâ€å°æ˜Ÿè€å¸ˆæŠŠé‚£ä¸ªæ­»äº¡çš„ä¸–ç•Œä¸¾åˆ°å­©å­ä»¬é¢å‰ã€‚\n",
      "\n",
      "â€œå®ƒå¤ªå°äº†ï¼â€\n",
      "\n",
      "â€œè¯´å¾—å¯¹ï¼Œå¤ªå°äº†ã€‚å°çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¸ç®¡å¤šä¹ˆç²¾ç¡®ï¼Œä¹Ÿæ˜¯ç»ä¸èµ·æ—¶é—´çš„é£æµªçš„ã€‚é£èˆ¹æ´¾æƒ³è±¡ä¸­çš„é£èˆ¹ä¹Ÿä¸€æ ·...\n",
      "doc #2: ç‰ˆæƒä¿¡æ¯\n",
      "\n",
      "\n",
      "ä¹¦åï¼šç§‘å¹»ä¸‰å·¨å¤´ç³»åˆ—ä¹‹æµæµªåœ°çƒ\n",
      "\n",
      "ä½œè€…ï¼šåˆ˜æ…ˆæ¬£ ç‹æ™‹åº· ä½•å¤•\n",
      "\n",
      "æ’ç‰ˆï¼šAGOOD\n",
      "\n",
      "ç¾ç¼–ï¼šå¸ƒé“ƒ\n",
      "\n",
      "ISBNï¼š9787547043158\n",
      "\n",
      "ç‰ˆæƒæ‰€æœ‰Â·ä¾µæƒå¿…ç©¶\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ä»æ­¦ä¾ çœ‹ä¸­å›½ç§‘å¹»ä¸‰...\n",
      "doc #3: â€œå¤ªæ£’äº†ï¼Œå…ƒå¸…è™«è™«ï¼ŒçœŸçš„å¤ªæ£’äº†ï¼â€å¤§ç‰™å¯¹å…ƒå¸…ç”±è¡·åœ°èµå¹ç€ï¼Œâ€œä¸è¿‡ä½ ä»¬è¦æŠ“ç´§ï¼Œåªå‰©ä¸‹ä¸€åœˆçš„åŠ é€Ÿæ—¶é—´äº†ï¼Œåé£Ÿå¸å›½å¯æ²¡æœ‰ç­‰å¾…åˆ«äººçš„ä¹ æƒ¯ã€‚æˆ‘è¿˜æœ‰ä¸ªç–‘é—®ï¼šä½ ä»¬åå¹´å‰å°±å·²å»ºæˆçš„åœ°ä¸‹åŸè¿˜ç©ºç€ï¼Œé‚£äº›ç§»æ°‘ä»€ä¹ˆæ—¶å€™æ¥ï¼Ÿä½ ...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from disk file\n",
    "loader = TextLoader('æµæµªåœ°çƒ.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# [5] Retriever can be directly accessed from vector store instance\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"æµæµªåœ°çƒè®¡åˆ’\")\n",
    "\n",
    "# [6] Interate around retrieved documents and print first 100 characters of each\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f'doc #{i}: {doc.page_content[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0779-3941-46ac-955e-944f708a23bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 LangChain: Hands-On ç»ƒä¹ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780749e-1242-4815-92ce-ec87d6617ed9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å€ŸåŠ©LangChainæ¡†æ¶æ„å»ºä¸€ä¸ªç®€å•çš„LLMåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬å³å°†æ„å»ºçš„åº”ç”¨ç¨‹åºæ˜¯ä¸€ä¸ªæ–‡æ¡£èŠå¤©æœºå™¨äººï¼Œå…è®¸æ‚¨å°±æ–‡æ¡£æ–‡ä»¶çš„å†…å®¹æå‡ºé—®é¢˜ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[Chatbot](https://python.langchain.com/docs/use_cases/chatbots)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a0149-d6a9-4ce2-bec8-17c549ba5bd4",
   "metadata": {},
   "source": [
    "**step1:**\n",
    "\n",
    "è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰è¦ä½¿ç”¨çš„LLMæ¨¡å‹ã€‚ä¸ä»¥å‰ä¸€æ ·ï¼Œå¯ä»¥ä½¿ç”¨æ™ºè°±AIã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b8eb571-9c5c-4df0-a945-0485da2806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model = 'chatglm_turbo', temperature = .7)\n",
    "\n",
    "template = (\n",
    "    \"Combine the chat history and follow up question into \"\n",
    "    \"a standalone question. Chat History: {chat_history}\"\n",
    "    \"Follow up question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab12d-7617-48ed-95fa-99058b316f5e",
   "metadata": {},
   "source": [
    "**step2:**\n",
    "  \n",
    "ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªç”¨äºå­˜å‚¨å†å²èŠå¤©æ¶ˆæ¯çš„è®°å¿†ï¼Œè¿™ä½¿å¾—èŠå¤©æœºå™¨äººèƒ½å¤Ÿè®°ä½å…ˆå‰çš„å¯¹è¯ã€‚åœ¨è¿™é‡Œï¼Œä¸å†ä½¿ç”¨ä¹‹å‰çš„ConversationBufferMemoryï¼Œè€Œæ˜¯å°è¯•å¦ä¸€ç§è®°å¿†ï¼Œå³ConversationSummaryMemoryã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db120e36-3347-4cf8-9eeb-7123b0e69f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = llm, \n",
    "                                   memory_key = 'chat_history', \n",
    "                                   return_messages = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0727dc7-45d1-41ab-b008-b2f2a3174ff9",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼ŒConversationSummaryMemoryæ¥å—ä¸€ä¸ªåä¸ºllmçš„å‚æ•°ã€‚\n",
    "\n",
    "å®é™…ä¸Šï¼Œè¿™ä¸ªè®°å¿†ä¿ç•™äº†ä¸¤ç§ç±»å‹çš„å†å²å¯¹è¯ä¿¡æ¯ï¼Œå³å†å²æ¶ˆæ¯çš„åˆ—è¡¨å’Œå†å²æ¶ˆæ¯çš„ç®€çŸ­æ‘˜è¦ã€‚\n",
    "\n",
    "ä¸ConversationBufferMemoryç›¸æ¯”ï¼Œæ‘˜è¦çš„ä½¿ç”¨ä½¿æˆ‘ä»¬ä¸ä¼šä½¿LLMä¸Šä¸‹æ–‡çª—å£ï¼ˆä»¤ç‰Œé™åˆ¶ï¼‰å˜å¾—è‡ƒè‚¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4d11-4386-4cb2-8460-ed7af7d0f60c",
   "metadata": {},
   "source": [
    "**step3:**\n",
    "\n",
    "ä¹‹åï¼Œè®©æˆ‘ä»¬å®Œæˆæ£€ç´¢å™¨éƒ¨åˆ†ï¼Œå³åŠ è½½æ–‡æ¡£ã€æ‹†åˆ†æ–‡æœ¬ã€è½¬æ¢ä¸ºåµŒå…¥å¹¶å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ã€‚\n",
    "  \n",
    "ä¸ä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨FAISSå‘é‡å­˜å‚¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fce06be-b6ea-477c-90f3-825d577168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadæ•°æ®èµ„æº\n",
    "# Write your code here\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from Internet\n",
    "blog_url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "loader = WebBaseLoader(blog_url)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dfa9dbe-8cb9-4bbe-a4e5-d1b542e891ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹†åˆ†æ•°æ®æˆå—\n",
    "# Write your code here\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92b684fe-7e26-431c-81a5-4ca01b7eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‘é‡å¤„ç†å­˜å…¥å‘é‡æ•°æ®åº“\n",
    "# Write your code here\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e21d4-6c61-4e48-a65b-e9647b662255",
   "metadata": {},
   "source": [
    "**step4:**\n",
    "æœ€åï¼Œè®©æˆ‘ä»¬å°†ä¸Šè¿°ç»„ä»¶ç»„åˆæˆä¸€ä¸ªå•ä¸€çš„é“¾ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é“¾æ˜¯`ConversationalRetrievalChain`ã€‚è¯¥é“¾çš„å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ä½¿ç”¨èŠå¤©å†å²å’Œæ–°é—®é¢˜åˆ›å»ºä¸€ä¸ªâ€œç‹¬ç«‹é—®é¢˜â€ã€‚\n",
    "2. å°†è¿™ä¸ªæ–°é—®é¢˜ä¼ é€’ç»™æ£€ç´¢å™¨ï¼Œå¹¶è¿”å›ç›¸å…³æ–‡æ¡£ã€‚\n",
    "3. å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸æ–°é—®é¢˜ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰æˆ–åŸå§‹é—®é¢˜å’ŒèŠå¤©å†å²ä¸€èµ·ä¼ é€’ç»™LLMï¼Œç”Ÿæˆæœ€ç»ˆçš„å“åº”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33837b92-bc2f-4a5e-aa56-7dbb6445af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm = llm, \n",
    "                                           memory = memory, \n",
    "                                           retriever = db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e45e2-ab5c-4677-b75f-d9f796b50081",
   "metadata": {},
   "source": [
    "**step5:**\n",
    "  \n",
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "295bb80e-f4c8-4ffe-ac01-4e2b97371748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How do agents use Task decomposition?\",\n",
      "  \"chat_history\": \"\\nsystem: \"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nsystem: \\nFollow Up Input: How do agents use Task decomposition?\\nStandalone question:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [2.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" What are the steps agents follow to implement task decomposition?\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [2.53s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" What are the steps agents follow to implement task decomposition?\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"\\\" What are the steps agents follow to implement task decomposition?\\\"\",\n",
      "  \"context\": \"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nQuestion: \\\" What are the steps agents follow to implement task decomposition?\\\"\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ZhipuAILLM] [21.00s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] [21.00s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] [21.00s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"\\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [24.14s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"\\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: How do agents use Task decomposition?\\nAI: \\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: How do agents use Task decomposition?\\nAI: \\\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\\\"\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [5.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "{'question': 'How do agents use Task decomposition?', 'chat_history': [SystemMessage(content='')], 'answer': '\" To implement task decomposition, agents follow these steps:\\\\n\\\\n1. Identify the main task: The agent first understands the overall task goal presented by the user.\\\\n2. Break down the task into smaller subtasks: The agent divides the main task into smaller, more manageable subtasks. These subtasks should be prioritized based on their importance and dependency on each other.\\\\n3. Determine the relationships between subtasks: The agent analyzes how the subtasks are connected and depends on one another to complete the main task. This helps in establishing a logical order for executing the subtasks.\\\\n4. Develop a plan: Based on the identified subtasks and their relationships, the agent creates a plan for executing the tasks. The plan should include resources, time requirements, and potential obstacles.\\\\n5. Allocate resources: The agent allocates necessary resources for each subtask, such as hardware, software, personnel, or external tools.\\\\n6. Monitor and adjust: During the execution of the tasks, the agent continuously monitors the progress and adjusts the plan accordingly to ensure smooth completion of the main task.\\\\n7. Integrate and validate the results: Once all the subtasks are completed, the agent integrates the results and validates them to ensure that the main task is fulfilled.\\\\n8. Iterate and improve: After the task completion, the agent reflects on the process and identifies areas of improvement for future tasks. This helps in refining the task decomposition process and increasing agent efficiency.\\\\n\\\\nThese steps enable agents to effectively decompose complex tasks into simpler, more manageable subtasks, ensuring smooth execution and accomplishment of the overall task goal.\"'}\n"
     ]
    }
   ],
   "source": [
    "# Question One: 'How do agents use Task decomposition?'\n",
    "# Write your code here.\n",
    "answer = qa(\"How do agents use Task decomposition?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5de5ec0-54bb-44cc-8f32-b0d8e4f57844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the various ways to implement memory to support it?\",\n",
      "  \"chat_history\": \"\\nsystem: \\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nsystem: \\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\\nFollow Up Input: What are the various ways to implement memory to support it?\\nStandalone question:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [2.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" What are the different methods for implementing memory support in task decomposition and planning for agents?\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:LLMChain] [2.35s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" What are the different methods for implementing memory support in task decomposition and planning for agents?\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"\\\" What are the different methods for implementing memory support in task decomposition and planning for agents?\\\"\",\n",
      "  \"context\": \"Fig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\nQuestion: \\\" What are the different methods for implementing memory support in task decomposition and planning for agents?\\\"\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain > 7:llm:ZhipuAILLM] [16.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain > 6:chain:LLMChain] [16.55s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 5:chain:StuffDocumentsChain] [16.55s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"\\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [19.47s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"\\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\",\n",
      "  \"new_lines\": \"Human: What are the various ways to implement memory to support it?\\nAI: \\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\\" The human asks about task decomposition and its use by agents. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal.\\\"\\n\\nNew lines of conversation:\\nHuman: What are the various ways to implement memory to support it?\\nAI: \\\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\\\n\\\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\\\n\\\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\\\n\\\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\\\n\\\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\\\"\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [10.44s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" The human asks about task decomposition and its use by agents, along with various memory implementation methods. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal. The AI also outlines three main methods for implementing memory support in task decomposition and planning for agents: short-term memory, long-term memory, and tool use. These methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals while improving their performance and adaptability.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [10.44s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" The human asks about task decomposition and its use by agents, along with various memory implementation methods. The AI explains that task decomposition involves breaking down a main task into smaller subtasks, prioritizing them, and establishing relationships between them. Agents then create a plan, allocate resources, monitor and adjust, integrate and validate the results, and iterate to improve the process. This ensures smooth execution and accomplishment of the overall task goal. The AI also outlines three main methods for implementing memory support in task decomposition and planning for agents: short-term memory, long-term memory, and tool use. These methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals while improving their performance and adaptability.\\\"\"\n",
      "}\n",
      "\" There are three main methods for implementing memory support in task decomposition and planning for agents:\\n\\n1. Short-term memory: This involves utilizing in-context learning, where the agent learns from the current context and uses that information to make decisions. In the case of LLM-powered agents, short-term memory refers to the memory mechanism employed by the transformer architecture's attention mechanism. This memory is finite and restricted by the context window length of the transformer.\\n\\n2. Long-term memory: This method involves storing and recalling information over extended periods. Agents leverage external vector stores and fast retrieval techniques to access this memory. Long-term memory enables agents to retain and recall information, allowing them to make more informed decisions and improve their performance.\\n\\n3. Tool use: Agents can utilize external APIs or other tools to obtain additional information that is missing from their model weights. This can include accessing current information, executing code, or accessing proprietary information sources. By doing so, agents can enhance their problem-solving capabilities and adapt to new situations.\\n\\nThese methods work together to enable agents to effectively decompose complex tasks into smaller, manageable subgoals. Additionally, agents can plan and execute these subgoals while leveraging memory to improve their performance and adapt to changing environments.\"\n"
     ]
    }
   ],
   "source": [
    "# Question Two: 'What are the various ways to implement memory to support it?'\n",
    "# Write your code here.\n",
    "print(qa('What are the various ways to implement memory to support it?')['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a79a5a-670c-425b-b287-a0bd77f12acc",
   "metadata": {},
   "source": [
    "## 3. åŸºäºLLMçš„Agentï¼ˆåŸºäºOpenAIï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe2e3-6903-46b8-ab75-99443ce79605",
   "metadata": {},
   "source": [
    "**Agent: Hands-On**\n",
    " \n",
    "Agentsçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨è¯­è¨€æ¨¡å‹é€‰æ‹©ä¸€ç³»åˆ—è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚\n",
    "\n",
    "è€Œåœ¨Chainsä¸­ï¼Œä¸€ç³»åˆ—åŠ¨ä½œæ˜¯ç¡¬ç¼–ç çš„ï¼ˆåœ¨ä»£ç ä¸­ï¼‰\n",
    "\n",
    "åœ¨Agentä¸­ï¼Œè¯­è¨€æ¨¡å‹è¢«ç”¨ä½œæ¨ç†å¼•æ“ï¼Œç¡®å®šè¦æ‰§è¡Œå“ªäº›åŠ¨ä½œä»¥åŠé¡ºåºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49329a-2433-49c0-8951-a2f8ae61443a",
   "metadata": {},
   "source": [
    "ä¸ºäº†æ”¯æŒæ„å»ºåŸºäºLLMçš„Agentï¼‰ï¼ŒLangChainæä¾›äº†ä»¥ä¸‹æ¨¡å—åŒ–ç»„ä»¶ï¼Œå³\n",
    "\n",
    "* `Tool`ï¼šåŒ…è£…äº†ä¸€ä¸ªPythonå‡½æ•°å’Œç›¸åº”çš„æ–‡æœ¬æè¿°ï¼Œå®ƒèµ‹äºˆAgentè°ƒç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œä¾‹å¦‚è®¡ç®—å™¨ã€Pythonè§£é‡Šå™¨ã€æœç´¢å¼•æ“APIã€‚\n",
    "* `Agent`ï¼šæ‰©å±•äº†æ™®é€šçš„LangChain`Chain`æ¨¡å—ï¼Œå…·æœ‰ä¸€ç»„`Tool`ï¼Œä»¥åŠç”¨äºä¸­é—´æ­¥éª¤çš„æç¤ºï¼ˆä¾‹å¦‚ReActä»£ç†çš„â€œæ€è€ƒ/åŠ¨ä½œ/è§‚å¯Ÿâ€è¿½è¸ªï¼‰ï¼Œä»£ç†æ‰§è¡Œçš„è¾“å‡ºè¦ä¹ˆæ˜¯è¦é‡‡å–çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼ˆ`AgentAction`ï¼‰ï¼Œè¦ä¹ˆæ˜¯å‘é€ç»™ç”¨æˆ·çš„æœ€ç»ˆå“åº”ï¼ˆ`AgentFinish`ï¼‰ã€‚\n",
    "* `AgentExecutor`ï¼šæ˜¯Agentçš„è¿è¡Œæ—¶ï¼Œå®ƒå®é™…ä¸Šè°ƒç”¨`Agent`ï¼Œæ‰§è¡Œå®ƒé€‰æ‹©çš„åŠ¨ä½œï¼Œå°†åŠ¨ä½œçš„è¾“å‡ºä¼ é€’å›Agentï¼Œç„¶åé‡å¤ï¼Œç›´åˆ°è¾¾åˆ°`AgentFinish`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2490-2e78-45ac-afaa-52fe2f4f7947",
   "metadata": {},
   "source": [
    "> â— å‡†å¤‡æ‚¨çš„APIå¯†é’¥\n",
    ">\n",
    "> ç¡®ä¿æ‚¨å·²ç»æŒ‰ç…§å…ˆå†³æ¡ä»¶è®¾ç½®äº†å¼€å‘ç¯å¢ƒï¼Œå¹¶æ‹¥æœ‰è°ƒç”¨LLMæœåŠ¡çš„æœ‰æ•ˆAPIå¯†é’¥ï¼Œè¿™é‡Œä»¥OpenAIä¸ºä¾‹ã€‚\n",
    ">\n",
    "> è¯·ç¡®ä¿æ‚¨å·²ç»ä»ç¯å¢ƒå˜é‡ä¸­åŠ è½½äº†OpenAPIå¯†é’¥ä»¥ä¾›ä½¿ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e0e5e-937d-4b6b-9236-556f2b54bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='replace_with_your_open_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56076b-6b0f-460a-a7af-39a34ca3154c",
   "metadata": {},
   "source": [
    "### 3.1 Tool: Python Function + Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7f6d-5f81-4074-a75b-8d3a6979f80f",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹LangChainç°æˆæä¾›çš„ä¸€äº›å†…ç½®Toolã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cc613-fb87-45e4-90ac-ea16e93d8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423193-4fee-44bc-9ee8-abacea1e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# [1] Some tools rely on LLM during its execution\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "\n",
    "math_tools = load_tools(['llm-math'], llm=llm)  # [1] Tool for arithmetic calculation\n",
    "meteo_tools = load_tools(['open-meteo-api'], llm=llm)  # [2] Tool for weather info\n",
    "wiki_tools = load_tools(['wikipedia'])  # [3] Tool for searching on Wikipedia\n",
    "\n",
    "# [4] Print total list of builtin tool names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c6b5-b8bb-4217-ab30-7133da36993d",
   "metadata": {},
   "source": [
    "#### `llm_math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ad41-7e01-4002-8c0f-3b8df5a84405",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math = math_tools[0]\n",
    "\n",
    "# [1] Try a simple equation.\n",
    "print(f'LLM Math: 2 + 2 => {llm_math.run(\"What is 2 + 2?\")}')\n",
    "\n",
    "# [2] How about a slightly diffucult one? Recall that pure LLM may fail on this example.\n",
    "print(f'LLM Math: (4829 + 2930) * 1923 => {llm_math.run(\"Sum 4829 and 2930, and then multiply by 1923.\")}')\n",
    "\n",
    "# [3] Pure LLM failed to reach the correct answer.\n",
    "print(f'Pure LLM: \\n{llm.predict(\"Sum 4829 and 2930, and then multiply by 1923.\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ec366-cb1a-4fa4-9547-54d17a3761c8",
   "metadata": {},
   "source": [
    "#### `open-meteo-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c7563-6a31-4333-a5b2-9312ab4ef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = meteo_tools[0]\n",
    "print(meteo.run(\"What's the weather in Paris?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c0e8e80-b1f6-4227-b5f2-3215f980ed18",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œé™¤äº†ä½¿ç”¨LangChainæä¾›çš„å†…ç½®Toolï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å®šä¹‰è‡ªå·±çš„å·¥å…·ï¼Œä»¥ä¾¿ä½¿ç”¨ç®€å•çš„ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfecfa-6da0-47b1-9f92-7e04a830b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool  # [1] We use the `tool` decorator to create new `Tool` instance\n",
    "def time(text: str) -> str:\n",
    "    # [2] The docstring (wrapped in \"\"\" \"\"\") are used as tool description\n",
    "    #     (which is sent to LLM when used by agent)\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())  # [3] The actual logic for this `Tool`, i.e, return today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412d745-de60-4321-86f4-a59a4e7fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.run('')  # Note the input is not used in our customed `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504426-27d8-4c3d-a143-cb6a419ae12c",
   "metadata": {},
   "source": [
    "å¦ä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·ï¼Œå®ƒæ¥å—å¤šä¸ªå‚æ•°ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªå•ä¸€çš„å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde986d1-1517-41fc-b214-f5f60baf8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str:\n",
    "    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n",
    "    result = requests.post(url, json=body, params=parameters)\n",
    "    return f\"Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9415-cd30-437b-b302-3fa458751a7d",
   "metadata": {},
   "source": [
    "### 3.2  Agent: Chain Equipped with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20e32a-9e08-4709-af24-ab0f22b97c17",
   "metadata": {},
   "source": [
    "LangChainå·²ç»å®šä¹‰äº†ä¸€äº›å†…ç½®çš„Agentç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨å…¶åŸºç¡€ä¸Šæ„å»ºæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe993ae0-9c7e-4be1-b853-24f2cc79c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.types import AgentType\n",
    "print([item.name for item in AgentType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7ce7e-c657-40f8-80a8-71aae8a9fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå³ZERO_SHOT_REACT_DESCRIPTIONï¼Œå®ƒç±»ä¼¼äºé›¶-shot ReActé£æ ¼çš„Agentã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4fb12-d468-4ea1-8b4b-65e2f1ada7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224d308-7681-475c-9c5e-f983ff6f1920",
   "metadata": {},
   "source": [
    "è¯·æ³¨æ„ï¼ŒLangChainä¸­çš„`Agent`æœ¬èº«ä¸è¿è¡Œï¼Œç›¸åï¼Œå®ƒå®šä¹‰äº†é€‚å½“çš„LLMã€å·¥å…·å’Œæç¤ºï¼Œåœ¨`AgentExecutor`ä¸­æ‰§è¡Œæ—¶ä½¿ç”¨ã€‚è®©æˆ‘ä»¬çœ‹çœ‹`ZeroShotAgent`æ˜¯å¦‚ä½•æ„å»ºå…¶æç¤ºçš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9616-15d7-4739-82e4-a8df23af4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bea3d-908f-45f7-8885-1934563f28ae",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œ`{input}` å®šä¹‰äº†ç”¨æˆ·è¾“å…¥æˆ–é—®é¢˜çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼Œâ€œå“ªæ”¯çƒé˜Ÿèµ¢å¾—äº†2022å¹´çš„FIFAä¸–ç•Œæ¯ï¼Ÿâ€ï¼›`{agent_scratchpad}` æ˜¯ä»£ç†å‘ˆç°å…¶è¿›ä¸€æ­¥æ‰§è¡Œçš„ä¸­é—´æ­¥éª¤çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼ŒReActä»£ç†çš„â€œæ€è€ƒ/åŠ¨ä½œ/è§‚å¯Ÿâ€ä¸‰å…ƒç»„åºåˆ—ã€‚æŒ‰è®¾è®¡ï¼Œåœ¨LangChainä¸­ï¼Œæ¯ä¸ª`Agent`éƒ½åº”è¯¥åœ¨å…¶æç¤ºæ¨¡æ¿ä¸­å®šä¹‰ä¸€ä¸ªå˜é‡`{agent_scratchpad}`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34599-40d1-4119-a8e5-63fb2a8e840a",
   "metadata": {},
   "source": [
    "### 3.3 AgentExecutor: Where Agents Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e704dc9-fc43-4e69-a41e-321cc17b6e6e",
   "metadata": {},
   "source": [
    "`AgentExecutor`æ˜¯`Agent`ï¼ˆå°±åƒæˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„é‚£æ ·ï¼‰å®é™…æ‰§è¡Œçš„åœ°æ–¹ã€‚æ ¹æ®æˆ‘ä»¬å¸Œæœ›ä»£ç†è¿è¡Œçš„æ–¹å¼ï¼Œå¯ä»¥æœ‰ä¸åŒç±»å‹çš„`AgentExecutor`ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨LangChainæä¾›çš„é»˜è®¤`AgentExecutor`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2be4-e2de-444d-bf65-244954077f6e",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¥è‡ª`AgentExecutor`ï¼Œå±•ç¤ºäº†LangChainä¸­é€šå¸¸å¦‚ä½•æ‰§è¡Œ`Agent`ã€‚\n",
    "```python\n",
    "class AgentExecutor(Chain):\n",
    "    ...\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        ...\n",
    "        # [1] To prevent `Agent`s from running into an infinite loop, `AgentExecutor` use\n",
    "        #     both number of LLM invocations (`iterations`) and used time (`time_elapsed`)\n",
    "        #     to stop execution even if `Agent` do not want to finish\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # [2] We now enter into the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            # [3] Take a single step in the \"Thought/Action/Observation\" loop, \n",
    "            #     return either `AgentAction` plus input or `AgentFinish`\n",
    "            next_step_output = self._take_next_step(...)\n",
    "            if isinstance(next_step_output, AgentFinish):  # [4] Return if LLM decides to finish\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "    \n",
    "            intermediate_steps.extend(next_step_output)  # [5] Store current step, i.e, `AgentAction` plus input\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:  # [6] Check the next `AgentAction` wants to return directly\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # [7] Deal with early stop, can still return something even if stopped in the middle\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e894-a713-425e-8bd6-f05e3da36a55",
   "metadata": {},
   "source": [
    "### 3.4 Put It Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb57e-c577-4954-a7c2-4bf07089347d",
   "metadata": {},
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬å°†`Tool`ã€`Agent`å’Œ`AgentExecutor`ç»“åˆèµ·æ¥ï¼Œçœ‹çœ‹LangChainä»£ç†æœ‰å“ªäº›åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990dd5d-478d-4ebf-87f5-0a4afcb2bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, AgentExecutor\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(executor.invoke('What is the weather in Berlin? Raise it to the power of 2.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce705842-df82-4862-abd2-7e44aabfc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
