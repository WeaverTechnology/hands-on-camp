{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ebed44-a2d2-4965-9048-7d4caece6d82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. 准备环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bdcf5-f21e-40f2-8979-548cb1db6ba5",
   "metadata": {},
   "source": [
    "### 1.1 安装依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66b4b-15f8-47fa-a6c0-70795799ad31",
   "metadata": {},
   "source": [
    "现在，让我们安装一些额外的库，例如 langchain 和 python-dotenv。\n",
    "\n",
    "前者为我们提供了一个构建基于LLM的应用程序的模块化框架，而后者在为在线LLM服务设置API密钥方面为我们节省了时间（有关详细信息，请参见下一节）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0608b18-2faf-40f8-beaf-efbd713e4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain==0.0.338 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.338)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (3.9.0)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (0.0.66)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain==0.0.338) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.338) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.338) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install langchain, the library we will learn during our courses\n",
    "!pip install langchain==0.0.338 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892e98f2-bbf1-4c8f-a755-5a8c64e9a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install dotenv, auto-load environment variables from `.env` files\n",
    "!pip install python-dotenv==1.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919c726-32d3-4025-9e8f-db00e5ca1c74",
   "metadata": {},
   "source": [
    "此外，让我们安装用于对内容进行标记化和存储在向量数据库上的库，即 tiktoken 和 faiss-cpu。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3e332c-0324-42ca-b5ca-8be73cfea0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tiktoken==0.5.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken==0.5.1) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken==0.5.1) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken, the library used by OpenAI models for tokenizing text strings\n",
    "!pip install tiktoken==0.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55858b86-2ff1-46a7-a473-78295437ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: faiss-cpu==1.7.4 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install faiss-cpu, a vector database for storing content along with embedding vectors\n",
    "!pip install faiss-cpu==1.7.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2865033a-0683-4b6a-a478-8f6b9d120765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f5f4-63d3-441d-8c9a-f7976f33555c",
   "metadata": {},
   "source": [
    "然后，安装一些用于访问外部服务的库，例如 wikipedia。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a14613b-82a4-40b2-8c82-770d5cb7b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d50-d9ac-44b3-b7c6-25b084474119",
   "metadata": {},
   "source": [
    "最后，为了测试安装和API密钥的有效性，我们还安装相应供应商的SDK库（即OpenAI和智谱AI）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2623724c-492a-4737-be44-0ae014214042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: openai==1.3.3 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (2.5.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai==1.3.3) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.3) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.3.3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.3.3) (2.14.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai==1.3.3) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install openai, official SDK by OpenAI for invoking GPT models\n",
    "!pip install openai==1.3.3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1338b17d-d953-4742-9fed-e8a2080170b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: zhipuai==1.0.7 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: requests in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zhipuai==1.0.7) (2.31.0)\n",
      "Requirement already satisfied: PyJWT in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zhipuai==1.0.7) (2.8.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zhipuai==1.0.7) (0.6)\n",
      "Requirement already satisfied: cachetools in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zhipuai==1.0.7) (5.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->zhipuai==1.0.7) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->zhipuai==1.0.7) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->zhipuai==1.0.7) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fradow\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->zhipuai==1.0.7) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install zhipu, official SDK by OpenAI for invoking ChatGLM models\n",
    "!pip install zhipuai==1.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972013-340f-427f-9038-12cde927c9a7",
   "metadata": {},
   "source": [
    "### 1.2 环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6cfdeb-fdbf-4a09-a8fa-0ed98e470606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:10.527350700Z",
     "start_time": "2023-11-26T13:10:10.508047600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZHIPUAI_API_KEY']='258f67eda0c7d05561dbaee91301b0d4.VZaOneqA6VuwpKT4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017855e-c925-4264-9319-e47ebcbe250b",
   "metadata": {},
   "source": [
    "### 1.3 测试准备是否成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af19cdeb-20c5-44aa-85b2-6df9422bf5ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:18.140150300Z",
     "start_time": "2023-11-26T13:10:15.963056200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 我的名字是简。你的名字叫什么？\"\n"
     ]
    }
   ],
   "source": [
    "# Test zhipuai installation\n",
    "import os\n",
    "import zhipuai\n",
    "\n",
    "zhipuai.api_key = '258f67eda0c7d05561dbaee91301b0d4.VZaOneqA6VuwpKT4'  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "completion = zhipuai.model_api.invoke(\n",
    "    model='chatglm_turbo',\n",
    "    prompt=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0.,\n",
    ")\n",
    "\n",
    "print(completion['data']['choices'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0678d-8625-41ce-b335-cb09274c868d",
   "metadata": {},
   "source": [
    "## 2. Langchain基础练习（基于智谱LLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07742-bfc3-4e59-888f-f5619a9fe247",
   "metadata": {},
   "source": [
    "与OpenAI不同，LangChain并不原生支持智谱AI的在线LLM服务。相反，我们可以编写一个包装类来将智谱AI的ChatGLP服务移植到LangChain，这要归功于LangChain的模块化接口。这应该类似于我们使用OpenAI的GPT服务时的感觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c752ae-14ff-451b-a30d-cb1190340da2",
   "metadata": {},
   "source": [
    "### 2.1 检查ZhipuAI wrapper是否存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c40040-163c-4df3-a869-15a4fa931043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:21.572827300Z",
     "start_time": "2023-11-26T13:10:21.525822700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# Check ZhipuAI wrapper existence\n",
    "!ls -la | grep \"zhipuai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e2d6-86c1-4900-8398-fa25a3559aea",
   "metadata": {},
   "source": [
    "### 2.2 简单使用例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bd3d2f-b446-42a6-9920-7ad218dfc003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:10:43.388499200Z",
     "start_time": "2023-11-26T13:10:25.686671100Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 我的名字是简。你叫什么名字？\"\n"
     ]
    }
   ],
   "source": [
    "import zhipuai\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "zhipuai.api_key = '258f67eda0c7d05561dbaee91301b0d4.VZaOneqA6VuwpKT4'  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88ab0-0ba3-4140-a939-ae91f39b4521",
   "metadata": {},
   "source": [
    "#### 练习1 - \"计算时间复杂度\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc11a5-6e42-4419-b3f6-ad8d0f2fa105",
   "metadata": {},
   "source": [
    "> 💪 Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> \n",
    "> ```\n",
    "> You will be provided with Python code, and your task is to calculate its time complexity.\n",
    ">\n",
    "> def foo(n, k):\n",
    ">    accum = 0\n",
    ">    for i in range(n):\n",
    ">        for l in range(k):\n",
    ">            accum += i\n",
    ">    return accum\n",
    "> ```\n",
    "> \n",
    "> ---------------------------\n",
    "> Try to change the Python code for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d167350b-bca4-4bd0-9a15-0f10e8d6187d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19a3a-dd46-464b-8612-c103dd0e3427",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 练习2 - “微博情感分析”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d45ed-cc60-4d04-a4c6-6719721e4889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> 💪 Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "> positive, neutral, or negative.\n",
    "> \n",
    "> I loved the new Batman movie!\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the tweet text for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e4b7ba-2bb0-46cc-9407-7855b101710c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a19-cabc-4801-9178-9024748a8a8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 练习3 - “机场代号提取”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf980e2b-c9c0-4cdf-8b44-062027f8fa8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> 💪 Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "> \n",
    "> I want to fly from Orlando to Boston\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the city names and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a10b019-a071-407a-8e4e-0fb13cdbc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09091de-7a0a-4338-918c-cca0608a2397",
   "metadata": {},
   "source": [
    "### 2.3 探索LLM局限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e6a1b5-e44e-47ff-9584-8d3040106eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:11:02.053709600Z",
     "start_time": "2023-11-26T13:10:52.294463900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1st response: \" The 1986 FIFA World Cup was won by Argentina, who defeated West Germany in the final match by a score of 3-2. Diego Maradona, who played for Argentina, scored the famous \\\"Hand of God\\\" goal in the match against England during the quarterfinals, which helped Argentina win the tournament.\"\n",
      "- 2nd response: \" As an AI language model, I cannot predict the future. The 2022 FIFA World Cup will take place in Qatar, but the winning team is yet to be determined. Keep an eye on the tournament to find out which team emerges victorious!\"\n"
     ]
    }
   ],
   "source": [
    "import zhipuai\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "zhipuai.api_key = '258f67eda0c7d05561dbaee91301b0d4.VZaOneqA6VuwpKT4'  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"Which team won the 1986 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 1st response: {response}')\n",
    "\n",
    "prompt = \"\"\"Which team won the 2022 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 2nd response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d69741-cf70-453b-abb8-b865aee1e07e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:11:35.423030200Z",
     "start_time": "2023-11-26T13:11:25.822327700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpt: \" First, let's add 4829 and 2930:\\n\\n4829 + 2930 = 7759\\n\\nNow, let's multiply the sum by 1923:\\n\\n7759 \\\\* 1923 = 146,421,427\\n\\nSo, (4829 + 2930) \\\\* 1923 = 146,421,427.\"\n",
      "- truth:\n",
      "\n",
      " 14920557\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Sum 4829 and 2930, and then multiply by 1923.\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(f'- gpt: {response}')\n",
    "print(f'- truth:\\n\\n {(4829 + 2930) * 1923}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15a3b-39e5-4420-b81e-75a4aed444e0",
   "metadata": {},
   "source": [
    "### 2.4 探索Langchain模块化组件设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72490709-13f7-4fd1-9fa6-6fed0c61e876",
   "metadata": {},
   "source": [
    "📌 打开调试和详细模式\n",
    "\n",
    "如果您是初学者，我们建议您在LangChain中打开调试和详细模式，在LLM应用程序执行过程中显示中间步骤的额外信息。\n",
    "查看提示如何填充以及中间LLM生成的响应是个好主意（在正常模式下不应打印任何输出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6857b586-03a6-4700-a1a5-83b3e034a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee3a0-2cf6-4dd8-9a4e-f1dfb70227db",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa2ad97-46ce-424a-afb7-a94b7ef674ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"colors\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] [3.12s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\\\" red, blue, green, yellow, purple\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\" red', 'blue', 'green', 'yellow', 'purple\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Custom output parser, split comma separated strings and return as list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# [2] System message template, declare task requirement as prompt\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# [3] Human message template, here we use Python format string syntax\n",
    "# (https://docs.python.org/3/library/string.html#formatstrings)\n",
    "human_template = '{text}'\n",
    "\n",
    "# [4] We send both messages to LLM for response\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# [5] Build up simple chain with LangChain Expression Language\n",
    "# (https://python.langchain.com/docs/expression_language/)\n",
    "chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()\n",
    "\n",
    "# [6] Call simple chain with human input, i.e., text = \"colors\"\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5c10-9981-474c-9ad2-288ede059f10",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40377-0d36-4250-9585-41dc0a63ea56",
   "metadata": {},
   "source": [
    "在接下来的部分，我们将专注于传统的Chain接口。首先开始重写前一节中的ICEL风格链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "652675c6-df3b-426e-b6b6-97efc878af1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:27:01.649304500Z",
     "start_time": "2023-11-26T13:26:58.685577400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [3.21s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3.21s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['\" red', 'blue', 'green', 'yellow', 'purple\"']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = '{text}'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# Equivalent to `chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()`\n",
    "chain = LLMChain(\n",
    "    llm=ZhipuAILLM(model='chatglm_turbo'),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d039d6c-ad90-4b0b-bbba-1a71588878d6",
   "metadata": {},
   "source": [
    "然后，让我们看一个更复杂的链。我们将介绍一个简单的两阶段连续链，其中：\n",
    "\n",
    "1. 为一家制造某种产品的公司提出名称\n",
    "2. 为提出的公司写一个简短的描述（即口号）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca8bcd6b-f5e7-44f0-90bd-192e352c91a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:27:15.347219300Z",
     "start_time": "2023-11-26T13:27:09.302104500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the best name to describe a company that makes Pure Milk?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [5.34s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\"Pure Milk Company,\\\\\\\" \\\\\\\"Clean Milk Enterprises,\\\\\\\" \\\\\\\"Wholesome Dairy Farms,\\\\\\\" or \\\\\\\"Natural Milk Processors.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk while highlighting its commitment to consumer health and wellness.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] [5.34s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\"Pure Milk Company,\\\\\\\" \\\\\\\"Clean Milk Enterprises,\\\\\\\" \\\\\\\"Wholesome Dairy Farms,\\\\\\\" or \\\\\\\"Natural Milk Processors.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk while highlighting its commitment to consumer health and wellness.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"company_name\": \"\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\"Pure Milk Company,\\\\\\\" \\\\\\\"Clean Milk Enterprises,\\\\\\\" \\\\\\\"Wholesome Dairy Farms,\\\\\\\" or \\\\\\\"Natural Milk Processors.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk while highlighting its commitment to consumer health and wellness.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a 20 words slogan for the following company:\\\" A company that makes pure milk can be named \\\\\\\"Pure Milk Producer,\\\\\\\" \\\\\\\"Pure Milk Company,\\\\\\\" \\\\\\\"Clean Milk Enterprises,\\\\\\\" \\\\\\\"Wholesome Dairy Farms,\\\\\\\" or \\\\\\\"Natural Milk Processors.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk while highlighting its commitment to consumer health and wellness.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] [2.24s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Experience the Pure Joy of Fresh, Wholesome Milk!\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] [2.24s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Experience the Pure Joy of Fresh, Wholesome Milk!\\\\\\\"\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] [7.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\\" \\\\\\\"Experience the Pure Joy of Fresh, Wholesome Milk!\\\\\\\"\\\"\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" \\\\\"Experience the Pure Joy of Fresh, Wholesome Milk!\\\\\"\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "product = 'Pure Milk'\n",
    "\n",
    "# [0] The same LLM instance shared by both chains (remember LLM is stateless)\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [1] Build name chain (1st chain)\n",
    "name_template = \"\"\"What is the best name to describe a company that makes {product}?\"\"\"\n",
    "name_prompt = ChatPromptTemplate.from_template(name_template)\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "# [2] Build slogan chain (2nd chain)\n",
    "slogan_template = \"\"\"Write a 20 words slogan for the following company:{company_name}\"\"\"\n",
    "slogan_prompt = ChatPromptTemplate.from_template(slogan_template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "# [3] Construct final chain in a sequencial manner\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, slogan_chain])\n",
    "\n",
    "# [4] Call our final chain to propose and write slogan\n",
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee8a2-5574-4f87-9d45-8f2161edd5b6",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dca0f-aef3-4a36-8edc-d68d55c40893",
   "metadata": {},
   "source": [
    "回顾一下我们说过的LLM本质上是无状态的，即后续调用永远不会回忆起在之前的调用中提到的信息。让我们看一个例子来说明这个说法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06801d81-a3f2-46e1-a7db-1a33bba1b53c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:31:03.751430900Z",
     "start_time": "2023-11-26T13:30:54.338153900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Hello, my name is Charles.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.88s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello Charles! It's nice to meet you. How can I assist you today? If you have any questions or need help with anything, feel free to ask.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Initial message: \" Hello Charles! It's nice to meet you. How can I assist you today? If you have any questions or need help with anything, feel free to ask.\"\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Well, what is my name?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.65s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" As an AI language model, I don't have real-time awareness or knowledge of your name. However, if you provide me with your name, I can help you with any questions or tasks you have.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Follow-up message: \" As an AI language model, I don't have real-time awareness or knowledge of your name. However, if you provide me with your name, I can help you with any questions or tasks you have.\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "print(f'Initial message: {llm.predict(\"Hello, my name is Charles.\")}')\n",
    "print(f'Follow-up message: {llm.predict(\"Well, what is my name?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d238-cf65-4356-bbc6-797acef6fed1",
   "metadata": {},
   "source": [
    "现在，让我们看看如何在LangChain中为一个对话应用程序添加一个记忆模块。具体来说，我们将使用ConversationBufferMemory记忆模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1731ea5-edd8-42c3-9a3b-a4d1ccc02913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:33:50.343089Z",
     "start_time": "2023-11-26T13:33:44.480663700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Hello, my name is Charles.\",\n",
      "  \"chat_history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\n\\n\\nNew human question: Hello, my name is Charles.\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [4.43s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [4.43s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance.\\\"\"\n",
      "}\n",
      "Initial message: \" Hello, Charles! It\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance.\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Well, what is my name?\",\n",
      "  \"chat_history\": \"Human: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance.\\\"\\n\\nNew human question: Well, what is my name?\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [3.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Charles, your name is Charles. You had previously introduced yourself as Charles, and I acknowledged your name in my greeting. If you have any other questions or want to discuss a different topic, feel free to ask.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Charles, your name is Charles. You had previously introduced yourself as Charles, and I acknowledged your name in my greeting. If you have any other questions or want to discuss a different topic, feel free to ask.\\\"\"\n",
      "}\n",
      "Follow-up message: \" Charles, your name is Charles. You had previously introduced yourself as Charles, and I acknowledged your name in my greeting. If you have any other questions or want to discuss a different topic, feel free to ask.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# [2] Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [3] Memory should work with Chain for effect\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b293b-93c1-4c4c-9b41-e0ecda1d0c31",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394b53-6399-4e64-9a51-a9544a62590e",
   "metadata": {},
   "source": [
    "现在，让我们看一个简单的检索方式，即基于向量存储的检索器，并看看它在LangChain组件中的工作原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "496c96be-ce08-4cb3-8b2c-6cc0d5b060a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T16:11:38.808676500Z",
     "start_time": "2023-11-26T16:09:30.186359600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc #0: 我用那只没受伤的手抽出手枪，随着这群突然狂热起来的受伤和没受伤的人，沿着钢铁通道，向地球驾驶室冲去。出乎意料，一路上我们几乎没遇到抵抗，倒是有越来越多的人从错综复杂的钢铁通道的各个分支中加入我们。最后...\n",
      "doc #1: “这个小世界死了。孩子们，谁能说出为什么？”小星老师把那个死亡的世界举到孩子们面前。\n",
      "\n",
      "“它太小了！”\n",
      "\n",
      "“说得对，太小了。小的生态系统，不管多么精确，也是经不起时间的风浪的。飞船派想象中的飞船也一样...\n",
      "doc #2: 版权信息\n",
      "\n",
      "\n",
      "书名：科幻三巨头系列之流浪地球\n",
      "\n",
      "作者：刘慈欣 王晋康 何夕\n",
      "\n",
      "排版：AGOOD\n",
      "\n",
      "美编：布铃\n",
      "\n",
      "ISBN：9787547043158\n",
      "\n",
      "版权所有·侵权必究\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从武侠看中国科幻三...\n",
      "doc #3: “太棒了，元帅虫虫，真的太棒了！”大牙对元帅由衷地赞叹着，“不过你们要抓紧，只剩下一圈的加速时间了，吞食帝国可没有等待别人的习惯。我还有个疑问：你们十年前就已建成的地下城还空着，那些移民什么时候来？你...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from disk file\n",
    "loader = TextLoader('wandering_earth.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# [5] Retriever can be directly accessed from vector store instance\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"流浪地球计划\")\n",
    "\n",
    "# [6] Interate around retrieved documents and print first 100 characters of each\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f'doc #{i}: {doc.page_content[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0779-3941-46ac-955e-944f708a23bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 LangChain: Hands-On 练习4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780749e-1242-4815-92ce-ec87d6617ed9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "在本节中，我们将借助LangChain框架构建一个简单的LLM应用程序。我们即将构建的应用程序是一个文档聊天机器人，允许您就文档文件的内容提出问题。有关更多信息，请参阅[Chatbot](https://python.langchain.com/docs/use_cases/chatbots)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a0149-d6a9-4ce2-bec8-17c549ba5bd4",
   "metadata": {},
   "source": [
    "**step1:**\n",
    "\n",
    "让我们首先定义要使用的LLM模型。与以前一样，可以使用智谱AI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b8eb571-9c5c-4df0-a945-0485da2806e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:40:47.154913500Z",
     "start_time": "2023-11-26T13:40:47.139272400Z"
    }
   },
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "the_llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab12d-7617-48ed-95fa-99058b316f5e",
   "metadata": {},
   "source": [
    "**step2:**\n",
    "  \n",
    "然后，创建一个用于存储历史聊天消息的记忆，这使得聊天机器人能够记住先前的对话。在这里，不再使用之前的ConversationBufferMemory，而是尝试另一种记忆，即ConversationSummaryMemory。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db120e36-3347-4cf8-9eeb-7123b0e69f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T13:40:51.346379200Z",
     "start_time": "2023-11-26T13:40:51.315663900Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = the_llm, memory_key='chat_history', return_messages=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0727dc7-45d1-41ab-b008-b2f2a3174ff9",
   "metadata": {},
   "source": [
    "注意，ConversationSummaryMemory接受一个名为llm的参数。\n",
    "\n",
    "实际上，这个记忆保留了两种类型的历史对话信息，即历史消息的列表和历史消息的简短摘要。\n",
    "\n",
    "与ConversationBufferMemory相比，摘要的使用使我们不会使LLM上下文窗口（令牌限制）变得臃肿。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4d11-4386-4cb2-8460-ed7af7d0f60c",
   "metadata": {},
   "source": [
    "**step3:**\n",
    "\n",
    "之后，让我们完成检索器部分，即加载文档、拆分文本、转换为嵌入并存储在数据库中。\n",
    "  \n",
    "与之前一样，我们将使用FAISS向量存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fce06be-b6ea-477c-90f3-825d577168eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T16:12:32.534545Z",
     "start_time": "2023-11-26T16:12:32.524540700Z"
    }
   },
   "outputs": [],
   "source": [
    "# load数据资源\n",
    "blog_url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(blog_url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dfa9dbe-8cb9-4bbe-a4e5-d1b542e891ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T16:12:37.325608800Z",
     "start_time": "2023-11-26T16:12:37.309597500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 拆分数据成块\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92b684fe-7e26-431c-81a5-4ca01b7eec58",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-26T16:12:40.834087300Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# 向量处理存入向量数据库\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e21d4-6c61-4e48-a65b-e9647b662255",
   "metadata": {},
   "source": [
    "**step4:**\n",
    "最后，让我们将上述组件组合成一个单一的链。我们使用的链是`ConversationalRetrievalChain`。该链的工作方式如下：\n",
    "\n",
    "1. 使用聊天历史和新问题创建一个“独立问题”。\n",
    "2. 将这个新问题传递给检索器，并返回相关文档。\n",
    "3. 将检索到的文档与新问题（默认行为）或原始问题和聊天历史一起传递给LLM，生成最终的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837b92-bc2f-4a5e-aa56-7dbb6445af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(the_llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e45e2-ab5c-4677-b75f-d9f796b50081",
   "metadata": {},
   "source": [
    "**step5:**\n",
    "  \n",
    "现在，让我们测试一下我们的聊天机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bb80e-f4c8-4ffe-ac01-4e2b97371748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question One: 'How do agents use Task decomposition?'\n",
    "qa('How do agents use Task decomposition?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5ec0-54bb-44cc-8f32-b0d8e4f57844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Two: 'What are the various ways to implement memory to support it?'\n",
    "qa('\"What are the various ways to implement memory to support it?\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a79a5a-670c-425b-b287-a0bd77f12acc",
   "metadata": {},
   "source": [
    "## 3. 基于LLM的Agent（基于OpenAI）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe2e3-6903-46b8-ab75-99443ce79605",
   "metadata": {},
   "source": [
    "**Agent: Hands-On**\n",
    " \n",
    "Agents的核心思想是使用语言模型选择一系列要执行的动作。\n",
    "\n",
    "而在Chains中，一系列动作是硬编码的（在代码中）\n",
    "\n",
    "在Agent中，语言模型被用作推理引擎，确定要执行哪些动作以及顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49329a-2433-49c0-8951-a2f8ae61443a",
   "metadata": {},
   "source": [
    "为了支持构建基于LLM的Agent），LangChain提供了以下模块化组件，即\n",
    "\n",
    "* `Tool`：包装了一个Python函数和相应的文本描述，它赋予Agent调用外部工具的能力，例如计算器、Python解释器、搜索引擎API。\n",
    "* `Agent`：扩展了普通的LangChain`Chain`模块，具有一组`Tool`，以及用于中间步骤的提示（例如ReAct代理的“思考/动作/观察”追踪），代理执行的输出要么是要采取的下一个动作（`AgentAction`），要么是发送给用户的最终响应（`AgentFinish`）。\n",
    "* `AgentExecutor`：是Agent的运行时，它实际上调用`Agent`，执行它选择的动作，将动作的输出传递回Agent，然后重复，直到达到`AgentFinish`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2490-2e78-45ac-afaa-52fe2f4f7947",
   "metadata": {},
   "source": [
    "> ❗ 准备您的API密钥\n",
    ">\n",
    "> 确保您已经按照先决条件设置了开发环境，并拥有调用LLM服务的有效API密钥，这里以OpenAI为例。\n",
    ">\n",
    "> 请确保您已经从环境变量中加载了OpenAPI密钥以供使用，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e0e5e-937d-4b6b-9236-556f2b54bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='replace_with_your_open_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56076b-6b0f-460a-a7af-39a34ca3154c",
   "metadata": {},
   "source": [
    "### 3.1 Tool: Python Function + Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7f6d-5f81-4074-a75b-8d3a6979f80f",
   "metadata": {},
   "source": [
    "首先，让我们看一下LangChain现成提供的一些内置Tool。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cc613-fb87-45e4-90ac-ea16e93d8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423193-4fee-44bc-9ee8-abacea1e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# [1] Some tools rely on LLM during its execution\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "\n",
    "math_tools = load_tools(['llm-math'], llm=llm)  # [1] Tool for arithmetic calculation\n",
    "meteo_tools = load_tools(['open-meteo-api'], llm=llm)  # [2] Tool for weather info\n",
    "wiki_tools = load_tools(['wikipedia'])  # [3] Tool for searching on Wikipedia\n",
    "\n",
    "# [4] Print total list of builtin tool names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c6b5-b8bb-4217-ab30-7133da36993d",
   "metadata": {},
   "source": [
    "#### `llm_math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ad41-7e01-4002-8c0f-3b8df5a84405",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math = math_tools[0]\n",
    "\n",
    "# [1] Try a simple equation.\n",
    "print(f'LLM Math: 2 + 2 => {llm_math.run(\"What is 2 + 2?\")}')\n",
    "\n",
    "# [2] How about a slightly diffucult one? Recall that pure LLM may fail on this example.\n",
    "print(f'LLM Math: (4829 + 2930) * 1923 => {llm_math.run(\"Sum 4829 and 2930, and then multiply by 1923.\")}')\n",
    "\n",
    "# [3] Pure LLM failed to reach the correct answer.\n",
    "print(f'Pure LLM: \\n{llm.predict(\"Sum 4829 and 2930, and then multiply by 1923.\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ec366-cb1a-4fa4-9547-54d17a3761c8",
   "metadata": {},
   "source": [
    "#### `open-meteo-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c7563-6a31-4333-a5b2-9312ab4ef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = meteo_tools[0]\n",
    "print(meteo.run(\"What's the weather in Paris?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c0e8e80-b1f6-4227-b5f2-3215f980ed18",
   "metadata": {},
   "source": [
    "然后，除了使用LangChain提供的内置Tool，我们还可以定义自己的工具，以便使用简单的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfecfa-6da0-47b1-9f92-7e04a830b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool  # [1] We use the `tool` decorator to create new `Tool` instance\n",
    "def time(text: str) -> str:\n",
    "    # [2] The docstring (wrapped in \"\"\" \"\"\") are used as tool description\n",
    "    #     (which is sent to LLM when used by agent)\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())  # [3] The actual logic for this `Tool`, i.e, return today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412d745-de60-4321-86f4-a59a4e7fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.run('')  # Note the input is not used in our customed `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504426-27d8-4c3d-a143-cb6a419ae12c",
   "metadata": {},
   "source": [
    "另一个自定义工具，它接受多个参数作为输入并返回一个单一的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde986d1-1517-41fc-b214-f5f60baf8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str:\n",
    "    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n",
    "    result = requests.post(url, json=body, params=parameters)\n",
    "    return f\"Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9415-cd30-437b-b302-3fa458751a7d",
   "metadata": {},
   "source": [
    "### 3.2  Agent: Chain Equipped with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20e32a-9e08-4709-af24-ab0f22b97c17",
   "metadata": {},
   "source": [
    "LangChain已经定义了一些内置的Agent类型，我们可以直接在其基础上构建我们的应用程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe993ae0-9c7e-4be1-b853-24f2cc79c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.types import AgentType\n",
    "print([item.name for item in AgentType])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee11e613ec55533",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "让我们看一个例子，即ZERO_SHOT_REACT_DESCRIPTION，它类似于零-shot ReAct风格的Agent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4fb12-d468-4ea1-8b4b-65e2f1ada7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224d308-7681-475c-9c5e-f983ff6f1920",
   "metadata": {},
   "source": [
    "请注意，LangChain中的`Agent`本身不运行，相反，它定义了适当的LLM、工具和提示，在`AgentExecutor`中执行时使用。让我们看看`ZeroShotAgent`是如何构建其提示的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9616-15d7-4739-82e4-a8df23af4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bea3d-908f-45f7-8885-1934563f28ae",
   "metadata": {},
   "source": [
    "注意，`{input}` 定义了用户输入或问题的位置，例如，“哪支球队赢得了2022年的FIFA世界杯？”；`{agent_scratchpad}` 是代理呈现其进一步执行的中间步骤的位置，例如，ReAct代理的“思考/动作/观察”三元组序列。按设计，在LangChain中，每个`Agent`都应该在其提示模板中定义一个变量`{agent_scratchpad}`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34599-40d1-4119-a8e5-63fb2a8e840a",
   "metadata": {},
   "source": [
    "### 3.3 AgentExecutor: Where Agents Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e704dc9-fc43-4e69-a41e-321cc17b6e6e",
   "metadata": {},
   "source": [
    "`AgentExecutor`是`Agent`（就像我们上面定义的那样）实际执行的地方。根据我们希望代理运行的方式，可以有不同类型的`AgentExecutor`。大多数情况下，我们希望使用LangChain提供的默认`AgentExecutor`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2be4-e2de-444d-bf65-244954077f6e",
   "metadata": {},
   "source": [
    "以下代码片段来自`AgentExecutor`，展示了LangChain中通常如何执行`Agent`。\n",
    "```python\n",
    "class AgentExecutor(Chain):\n",
    "    ...\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        ...\n",
    "        # [1] To prevent `Agent`s from running into an infinite loop, `AgentExecutor` use\n",
    "        #     both number of LLM invocations (`iterations`) and used time (`time_elapsed`)\n",
    "        #     to stop execution even if `Agent` do not want to finish\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # [2] We now enter into the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            # [3] Take a single step in the \"Thought/Action/Observation\" loop, \n",
    "            #     return either `AgentAction` plus input or `AgentFinish`\n",
    "            next_step_output = self._take_next_step(...)\n",
    "            if isinstance(next_step_output, AgentFinish):  # [4] Return if LLM decides to finish\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "    \n",
    "            intermediate_steps.extend(next_step_output)  # [5] Store current step, i.e, `AgentAction` plus input\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:  # [6] Check the next `AgentAction` wants to return directly\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # [7] Deal with early stop, can still return something even if stopped in the middle\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e894-a713-425e-8bd6-f05e3da36a55",
   "metadata": {},
   "source": [
    "### 3.4 Put It Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb57e-c577-4954-a7c2-4bf07089347d",
   "metadata": {},
   "source": [
    "现在让我们将`Tool`、`Agent`和`AgentExecutor`结合起来，看看LangChain代理有哪些功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990dd5d-478d-4ebf-87f5-0a4afcb2bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, AgentExecutor\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(executor.invoke('What is the weather in Berlin? Raise it to the power of 2.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce705842-df82-4862-abd2-7e44aabfc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e137d4e5d44e440344600b184767fd7d5cf1dd2d5524c3d2365e9863ffb72fed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
