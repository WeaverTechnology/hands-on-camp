{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ebed44-a2d2-4965-9048-7d4caece6d82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. ÂáÜÂ§áÁéØÂ¢É"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bdcf5-f21e-40f2-8979-548cb1db6ba5",
   "metadata": {},
   "source": [
    "### 1.1 ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66b4b-15f8-47fa-a6c0-70795799ad31",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÂÆâË£Ö‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÂ∫ìÔºå‰æãÂ¶Ç langchain Âíå python-dotenv„ÄÇ\n",
    "\n",
    "ÂâçËÄÖ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊûÑÂª∫Âü∫‰∫éLLMÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÊ®°ÂùóÂåñÊ°ÜÊû∂ÔºåËÄåÂêéËÄÖÂú®‰∏∫Âú®Á∫øLLMÊúçÂä°ËÆæÁΩÆAPIÂØÜÈí•ÊñπÈù¢‰∏∫Êàë‰ª¨ËäÇÁúÅ‰∫ÜÊó∂Èó¥ÔºàÊúâÂÖ≥ËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇËßÅ‰∏ã‰∏ÄËäÇÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0608b18-2faf-40f8-beaf-efbd713e4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain==0.0.338 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.338)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (3.9.0)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (0.0.66)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.338) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (3.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.338) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.338) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# Install langchain, the library we will learn during our courses\n",
    "!pip install langchain==0.0.338 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "892e98f2-bbf1-4c8f-a755-5a8c64e9a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dotenv, auto-load environment variables from `.env` files\n",
    "!pip install python-dotenv==1.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919c726-32d3-4025-9e8f-db00e5ca1c74",
   "metadata": {},
   "source": [
    "Ê≠§Â§ñÔºåËÆ©Êàë‰ª¨ÂÆâË£ÖÁî®‰∫éÂØπÂÜÖÂÆπËøõË°åÊ†áËÆ∞ÂåñÂíåÂ≠òÂÇ®Âú®ÂêëÈáèÊï∞ÊçÆÂ∫ì‰∏äÁöÑÂ∫ìÔºåÂç≥ tiktoken Âíå faiss-cpu„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3996c2fa-80cd-434b-bfc3-557eef3ac37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: poetry in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: build<2.0.0,>=1.0.3 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.0.3)\n",
      "Requirement already satisfied: cachecontrol<0.14.0,>=0.13.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (0.13.1)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.1.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (2.1.0)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (0.21.6)\n",
      "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (2.19.0)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: keyring<25.0.0,>=24.0.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (24.3.0)\n",
      "Requirement already satisfied: packaging>=20.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (23.2)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (3.11.0)\n",
      "Requirement already satisfied: poetry-core==1.8.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.8.1)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.6.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.6.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (1.5.4)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (0.12.3)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (2023.11.22)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.23.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from poetry) (20.24.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build<2.0.0,>=1.0.3->poetry) (0.4.6)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cachecontrol<0.14.0,>=0.13.0->cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (1.0.7)\n",
      "Requirement already satisfied: filelock>=3.8.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (3.13.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cleo<3.0.0,>=2.1.0->poetry) (3.5.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.1.0)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.2.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (3.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (2023.11.17)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from virtualenv<21.0.0,>=20.23.0->poetry) (0.3.7)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install poetry -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c3e332c-0324-42ca-b5ca-8be73cfea0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/bd/ef/91777d3310589c55da4bf0fafa10fdc8ddefa30aa7dfa67b2fc8825bc1f1/tiktoken-0.5.1.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/d3/10/6f2d5f8635d7714ad97ce6ade7a643358c4f3e45cde4ed12b7150734a8f3/regex-2023.10.3-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhiweic\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Building wheels for collected packages: tiktoken\n",
      "  Building wheel for tiktoken (pyproject.toml): started\n",
      "  Building wheel for tiktoken (pyproject.toml): finished with status 'error'\n",
      "Failed to build tiktoken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tiktoken (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [38 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-312\n",
      "  creating build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\core.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\load.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\model.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\registry.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\_educational.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  copying tiktoken\\__init__.py -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  creating build\\lib.win-amd64-cpython-312\\tiktoken_ext\n",
      "  copying tiktoken_ext\\openai_public.py -> build\\lib.win-amd64-cpython-312\\tiktoken_ext\n",
      "  running egg_info\n",
      "  writing tiktoken.egg-info\\PKG-INFO\n",
      "  writing dependency_links to tiktoken.egg-info\\dependency_links.txt\n",
      "  writing requirements to tiktoken.egg-info\\requires.txt\n",
      "  writing top-level names to tiktoken.egg-info\\top_level.txt\n",
      "  reading manifest file 'tiktoken.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching 'Makefile'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'tiktoken.egg-info\\SOURCES.txt'\n",
      "  copying tiktoken\\py.typed -> build\\lib.win-amd64-cpython-312\\tiktoken\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tiktoken\n",
      "ERROR: Could not build wheels for tiktoken, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "# Install tiktoken, the library used by OpenAI models for tokenizing text strings\n",
    "!pip install tiktoken -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55858b86-2ff1-46a7-a473-78295437ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install faiss-cpu, a vector database for storing content along with embedding vectors\n",
    "!pip install faiss-cpu==1.7.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2865033a-0683-4b6a-a478-8f6b9d120765",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f5f4-63d3-441d-8c9a-f7976f33555c",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÂÆâË£Ö‰∏Ä‰∫õÁî®‰∫éËÆøÈóÆÂ§ñÈÉ®ÊúçÂä°ÁöÑÂ∫ìÔºå‰æãÂ¶Ç wikipedia„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a14613b-82a4-40b2-8c82-770d5cb7b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d50-d9ac-44b3-b7c6-25b084474119",
   "metadata": {},
   "source": [
    "ÊúÄÂêéÔºå‰∏∫‰∫ÜÊµãËØïÂÆâË£ÖÂíåAPIÂØÜÈí•ÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨ËøòÂÆâË£ÖÁõ∏Â∫î‰æõÂ∫îÂïÜÁöÑSDKÂ∫ìÔºàÂç≥OpenAIÂíåÊô∫Ë∞±AIÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2623724c-492a-4737-be44-0ae014214042",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install openai, official SDK by OpenAI for invoking GPT models\n",
    "!pip install openai==1.3.3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1338b17d-d953-4742-9fed-e8a2080170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install zhipu, official SDK by OpenAI for invoking ChatGLM models\n",
    "!pip install zhipuai==1.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972013-340f-427f-9038-12cde927c9a7",
   "metadata": {},
   "source": [
    "### 1.2 ÁéØÂ¢ÉÂèòÈáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8ce025-80bc-43b1-84fc-73b2f396ba3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2605503247.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    ECHO ZHIPUAI_API_KEY=<3fb52888d94d1ff2b72ac95bfd269a9e.BVSfK2CHbEb8sU9u > .env\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# Example `.env` config file.\n",
    "ECHO ZHIPUAI_API_KEY=<3fb52888d94d1ff2b72ac95bfd269a9e.BVSfK2CHbEb8sU9u > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6cfdeb-fdbf-4a09-a8fa-0ed98e470606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ZHIPUAI API key from `.env` file.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017855e-c925-4264-9319-e47ebcbe250b",
   "metadata": {},
   "source": [
    "### 1.3 ÊµãËØïÂáÜÂ§áÊòØÂê¶ÊàêÂäü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af19cdeb-20c5-44aa-85b2-6df9422bf5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ÊàëÁöÑÂêçÂ≠óÊòØÁÆÄ„ÄÇ‰Ω†ÁöÑÂêçÂ≠óÂè´‰ªÄ‰πàÔºü\"\n"
     ]
    }
   ],
   "source": [
    "# Test zhipuai installation\n",
    "import os\n",
    "import zhipuai\n",
    "\n",
    "zhipuai.api_key = os.getenv('ZHIPUAI_API_KEY')  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "completion = zhipuai.model_api.invoke(\n",
    "    model='chatglm_turbo',\n",
    "    prompt=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0.,\n",
    ")\n",
    "\n",
    "print(completion['data']['choices'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0678d-8625-41ce-b335-cb09274c868d",
   "metadata": {},
   "source": [
    "## 2. LangchainÂü∫Á°ÄÁªÉ‰π†ÔºàÂü∫‰∫éÊô∫Ë∞±LLMÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07742-bfc3-4e59-888f-f5619a9fe247",
   "metadata": {},
   "source": [
    "‰∏éOpenAI‰∏çÂêåÔºåLangChainÂπ∂‰∏çÂéüÁîüÊîØÊåÅÊô∫Ë∞±AIÁöÑÂú®Á∫øLLMÊúçÂä°„ÄÇÁõ∏ÂèçÔºåÊàë‰ª¨ÂèØ‰ª•ÁºñÂÜô‰∏Ä‰∏™ÂåÖË£ÖÁ±ªÊù•Â∞ÜÊô∫Ë∞±AIÁöÑChatGLPÊúçÂä°ÁßªÊ§çÂà∞LangChainÔºåËøôË¶ÅÂΩíÂäü‰∫éLangChainÁöÑÊ®°ÂùóÂåñÊé•Âè£„ÄÇËøôÂ∫îËØ•Á±ª‰ºº‰∫éÊàë‰ª¨‰ΩøÁî®OpenAIÁöÑGPTÊúçÂä°Êó∂ÁöÑÊÑüËßâ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c752ae-14ff-451b-a30d-cb1190340da2",
   "metadata": {},
   "source": [
    "### 2.1 Ê£ÄÊü•ZhipuAI wrapperÊòØÂê¶Â≠òÂú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4c40040-163c-4df3-a869-15a4fa931043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' ‰∏çÊòØÂÜÖÈÉ®ÊàñÂ§ñÈÉ®ÂëΩ‰ª§Ôºå‰πü‰∏çÊòØÂèØËøêË°åÁöÑÁ®ãÂ∫è\n",
      "ÊàñÊâπÂ§ÑÁêÜÊñá‰ª∂„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "# Check ZhipuAI wrapper existence\n",
    "!ls -la | grep \"zhipuai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e2d6-86c1-4900-8398-fa25a3559aea",
   "metadata": {},
   "source": [
    "### 2.2 ÁÆÄÂçï‰ΩøÁî®‰æãÂ≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18bd3d2f-b446-42a6-9920-7ad218dfc003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ÊàëÂè´ÁÆÄ„ÄÇ‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88ab0-0ba3-4140-a939-ae91f39b4521",
   "metadata": {},
   "source": [
    "#### ÁªÉ‰π†1 - \"ËÆ°ÁÆóÊó∂Èó¥Â§çÊùÇÂ∫¶\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc11a5-6e42-4419-b3f6-ad8d0f2fa105",
   "metadata": {},
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> \n",
    "> ```\n",
    "> You will be provided with Python code, and your task is to calculate its time complexity.\n",
    ">\n",
    "> def foo(n, k):\n",
    ">    accum = 0\n",
    ">    for i in range(n):\n",
    ">        for l in range(k):\n",
    ">            accum += i\n",
    ">    return accum\n",
    "> ```\n",
    "> \n",
    "> ---------------------------\n",
    "> Try to change the Python code for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d167350b-bca4-4bd0-9a15-0f10e8d6187d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ËøôÊÆµPython‰ª£Á†ÅÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫ O(n * k)ÔºåÂÖ∂‰∏≠nÊòØÂ§ñÈÉ®Âæ™ÁéØÁöÑÊ¨°Êï∞ÔºåkÊòØÂÜÖÈÉ®Âæ™ÁéØÁöÑÊ¨°Êï∞„ÄÇÂú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÂÜÖÈÉ®Âæ™ÁéØÁöÑÊ¨°Êï∞kÁ≠â‰∫énÔºåÊâÄ‰ª•Êó∂Èó¥Â§çÊùÇÂ∫¶ÂèØ‰ª•ÁÆÄÂåñ‰∏∫O(n^2)„ÄÇËøôÊÑèÂë≥ÁùÄÂΩìnÂ¢ûÂ§ßÊó∂ÔºåÂáΩÊï∞ÊâßË°åÊó∂Èó¥Â∞Ü‰ª•nÁöÑÂπ≥ÊñπÈÄüÂ∫¶Â¢ûÂä†„ÄÇ\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with Python code, and your task is to calculate its time complexity.\n",
    "\n",
    "def foo(n, k):\n",
    "   accum = 0\n",
    "   for i in range(n):\n",
    "       for l in range(k):\n",
    "           accum += i\n",
    "   return accum\n",
    "   respond in chinese\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19a3a-dd46-464b-8612-c103dd0e3427",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†2 - ‚ÄúÂæÆÂçöÊÉÖÊÑüÂàÜÊûê‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d45ed-cc60-4d04-a4c6-6719721e4889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "> positive, neutral, or negative.\n",
    "> \n",
    "> I loved the new Batman movie!\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the tweet text for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e4b7ba-2bb0-46cc-9407-7855b101710c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Ê≠£Èù¢\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "positive, neutral, or negative.\n",
    "\n",
    "I loved the new Batman movie!\n",
    "   respond answer in chinese\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a19-cabc-4801-9178-9024748a8a8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†3 - ‚ÄúÊú∫Âú∫‰ª£Âè∑ÊèêÂèñ‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf980e2b-c9c0-4cdf-8b44-062027f8fa8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "> \n",
    "> I want to fly from Orlando to Boston\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the city names and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a10b019-a071-407a-8e4e-0fb13cdbc2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÁöÑÊú∫Âú∫‰ª£Á†Å‰∏∫ÔºöMCOÔºàÂ••ÂÖ∞Â§öÊú∫Âú∫‰ª£Á†ÅÔºâÂíå BOSÔºàÊ≥¢Â£´È°øÊú∫Âú∫‰ª£Á†ÅÔºâ„ÄÇ\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "\n",
    "I want to fly from Orlando to Boston\n",
    "   respond answer in chinese\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09091de-7a0a-4338-918c-cca0608a2397",
   "metadata": {},
   "source": [
    "### 2.3 Êé¢Á¥¢LLMÂ±ÄÈôê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4e6a1b5-e44e-47ff-9584-8d3040106eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Which team won the 1986 FIFA World Cup? respond answer in chinese\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.92s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" 1986Âπ¥FIFA‰∏ñÁïåÊùØÁöÑÂÜ†ÂÜõÊòØÈòøÊ†πÂª∑Èòü„ÄÇ‰∏≠ÊñáÂõûÁ≠îÔºöÈòøÊ†πÂª∑ÈòüËµ¢Âæó‰∫Ü1986Âπ¥ÁöÑFIFA‰∏ñÁïåÊùØ„ÄÇ\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "- 1st response: \" 1986Âπ¥FIFA‰∏ñÁïåÊùØÁöÑÂÜ†ÂÜõÊòØÈòøÊ†πÂª∑Èòü„ÄÇ‰∏≠ÊñáÂõûÁ≠îÔºöÈòøÊ†πÂª∑ÈòüËµ¢Âæó‰∫Ü1986Âπ¥ÁöÑFIFA‰∏ñÁïåÊùØ„ÄÇ\"\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Which team won the 2022 FIFA World Cup?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.24s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" As an AI language model, I cannot predict future events. The 2022 FIFA World Cup will take place in Qatar, but the winner is yet to be determined as the tournament has not occurred. Stay tuned for updates!\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "- 2nd response: \" As an AI language model, I cannot predict future events. The 2022 FIFA World Cup will take place in Qatar, but the winner is yet to be determined as the tournament has not occurred. Stay tuned for updates!\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Which team won the 1986 FIFA World Cup? respond answer in chinese\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 1st response: {response}')\n",
    "\n",
    "prompt = \"\"\"Which team won the 2022 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 2nd response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1d69741-cf70-453b-abb8-b865aee1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpt: \" È¶ñÂÖàÂ∞Ü4829Âíå2930Áõ∏Âä†ÔºåÂæóÂà∞7759„ÄÇÁÑ∂ÂêéÂ∞Ü7759‰πò‰ª•1923ÔºåÁªìÊûú‰∏∫146,802,247„ÄÇÊâÄ‰ª•Ôºå(4829+2930)√ó1923=146,802,247„ÄÇ\"\n",
      "- truth:\n",
      "\n",
      " 14920557\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Sum 4829 and 2930, and then multiply by 1923.respond answer in chinese\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(f'- gpt: {response}')\n",
    "print(f'- truth:\\n\\n {(4829 + 2930) * 1923}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15a3b-39e5-4420-b81e-75a4aed444e0",
   "metadata": {},
   "source": [
    "### 2.4 Êé¢Á¥¢LangchainÊ®°ÂùóÂåñÁªÑ‰ª∂ËÆæËÆ°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72490709-13f7-4fd1-9fa6-6fed0c61e876",
   "metadata": {},
   "source": [
    "üìå ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°Âºè\n",
    "\n",
    "Â¶ÇÊûúÊÇ®ÊòØÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®Âú®LangChain‰∏≠ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°ÂºèÔºåÂú®LLMÂ∫îÁî®Á®ãÂ∫èÊâßË°åËøáÁ®ã‰∏≠ÊòæÁ§∫‰∏≠Èó¥Ê≠•È™§ÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ\n",
    "Êü•ÁúãÊèêÁ§∫Â¶Ç‰ΩïÂ°´ÂÖÖ‰ª•Âèä‰∏≠Èó¥LLMÁîüÊàêÁöÑÂìçÂ∫îÊòØ‰∏™Â•Ω‰∏ªÊÑèÔºàÂú®Ê≠£Â∏∏Ê®°Âºè‰∏ã‰∏çÂ∫îÊâìÂç∞‰ªª‰ΩïËæìÂá∫Ôºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6857b586-03a6-4700-a1a5-83b3e034a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee3a0-2cf6-4dd8-9a4e-f1dfb70227db",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afa2ad97-46ce-424a-afb7-a94b7ef674ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"colors\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] [4.09s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\\\" red, blue, green, yellow, purple\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [4.09s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\" red', 'blue', 'green', 'yellow', 'purple\"']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Custom output parser, split comma separated strings and return as list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# [2] System message template, declare task requirement as prompt\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# [3] Human message template, here we use Python format string syntax\n",
    "# (https://docs.python.org/3/library/string.html#formatstrings)\n",
    "human_template = '{text}'\n",
    "\n",
    "# [4] We send both messages to LLM for response\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# [5] Build up simple chain with LangChain Expression Language\n",
    "# (https://python.langchain.com/docs/expression_language/)\n",
    "chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()\n",
    "\n",
    "# [6] Call simple chain with human input, i.e., text = \"colors\"\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5c10-9981-474c-9ad2-288ede059f10",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40377-0d36-4250-9585-41dc0a63ea56",
   "metadata": {},
   "source": [
    "Âú®Êé•‰∏ãÊù•ÁöÑÈÉ®ÂàÜÔºåÊàë‰ª¨Â∞Ü‰∏ìÊ≥®‰∫é‰º†ÁªüÁöÑChainÊé•Âè£„ÄÇÈ¶ñÂÖàÂºÄÂßãÈáçÂÜôÂâç‰∏ÄËäÇ‰∏≠ÁöÑICELÈ£éÊ†ºÈìæ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652675c6-df3b-426e-b6b6-97efc878af1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\" red', 'blue', 'green', 'yellow', 'purple\"']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = '{text}'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# Equivalent to `chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()`\n",
    "chain = LLMChain(\n",
    "    llm=ZhipuAILLM(model='chatglm_turbo'),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d039d6c-ad90-4b0b-bbba-1a71588878d6",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™Êõ¥Â§çÊùÇÁöÑÈìæ„ÄÇÊàë‰ª¨Â∞Ü‰ªãÁªç‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰∏§Èò∂ÊÆµËøûÁª≠ÈìæÔºåÂÖ∂‰∏≠Ôºö\n",
    "\n",
    "1. ‰∏∫‰∏ÄÂÆ∂Âà∂ÈÄ†ÊüêÁßç‰∫ßÂìÅÁöÑÂÖ¨Âè∏ÊèêÂá∫ÂêçÁß∞\n",
    "2. ‰∏∫ÊèêÂá∫ÁöÑÂÖ¨Âè∏ÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑÊèèËø∞ÔºàÂç≥Âè£Âè∑Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca8bcd6b-f5e7-44f0-90bd-192e352c91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the best name to describe a company that makes Pure Milk? answer in chinese\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [3.29s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Á∫ØÂáÄÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÔºàPure Milk Products CompanyÔºâ\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] [3.29s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Á∫ØÂáÄÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÔºàPure Milk Products CompanyÔºâ\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"company_name\": \"\\\" Á∫ØÂáÄÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÔºàPure Milk Products CompanyÔºâ\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a 20 words slogan for the following company:\\\" Á∫ØÂáÄÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÔºàPure Milk Products CompanyÔºâ\\\" answer in chinese\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] [2.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Á∫ØÂáÄ‰πãÊ∫êÔºåÂÅ•Â∫∑ÂÆàÊä§ÔºåÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÁöÑÊ†áËØ≠Ôºö‚ÄúÁ∫ØÂáÄÂ•∂Ê∫êÔºåÂìÅË¥®‰º†Êâø‚Äù„ÄÇ\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] [2.55s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Á∫ØÂáÄ‰πãÊ∫êÔºåÂÅ•Â∫∑ÂÆàÊä§ÔºåÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÁöÑÊ†áËØ≠Ôºö‚ÄúÁ∫ØÂáÄÂ•∂Ê∫êÔºåÂìÅË¥®‰º†Êâø‚Äù„ÄÇ\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] [5.84s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\\" Á∫ØÂáÄ‰πãÊ∫êÔºåÂÅ•Â∫∑ÂÆàÊä§ÔºåÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÁöÑÊ†áËØ≠Ôºö‚ÄúÁ∫ØÂáÄÂ•∂Ê∫êÔºåÂìÅË¥®‰º†Êâø‚Äù„ÄÇ\\\"\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" Á∫ØÂáÄ‰πãÊ∫êÔºåÂÅ•Â∫∑ÂÆàÊä§ÔºåÂ•∂Âà∂ÂìÅÂÖ¨Âè∏ÁöÑÊ†áËØ≠Ôºö‚ÄúÁ∫ØÂáÄÂ•∂Ê∫êÔºåÂìÅË¥®‰º†Êâø‚Äù„ÄÇ\"'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "product = 'Pure Milk'\n",
    "\n",
    "# [0] The same LLM instance shared by both chains (remember LLM is stateless)\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [1] Build name chain (1st chain)\n",
    "name_template = \"\"\"What is the best name to describe a company that makes {product}? answer in chinese\"\"\"\n",
    "name_prompt = ChatPromptTemplate.from_template(name_template)\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "# [2] Build slogan chain (2nd chain)\n",
    "slogan_template = \"\"\"Write a 20 words slogan for the following company:{company_name} answer in chinese\"\"\"\n",
    "slogan_prompt = ChatPromptTemplate.from_template(slogan_template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "# [3] Construct final chain in a sequencial manner\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, slogan_chain])\n",
    "\n",
    "# [4] Call our final chain to propose and write slogan\n",
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee8a2-5574-4f87-9d45-8f2161edd5b6",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dca0f-aef3-4a36-8edc-d68d55c40893",
   "metadata": {},
   "source": [
    "ÂõûÈ°æ‰∏Ä‰∏ãÊàë‰ª¨ËØ¥ËøáÁöÑLLMÊú¨Ë¥®‰∏äÊòØÊó†Áä∂ÊÄÅÁöÑÔºåÂç≥ÂêéÁª≠Ë∞ÉÁî®Ê∞∏Ëøú‰∏ç‰ºöÂõûÂøÜËµ∑Âú®‰πãÂâçÁöÑË∞ÉÁî®‰∏≠ÊèêÂà∞ÁöÑ‰ø°ÊÅØ„ÄÇËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÊù•ËØ¥ÊòéËøô‰∏™ËØ¥Ê≥ï„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06801d81-a3f2-46e1-a7db-1a33bba1b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Hello, my name is Charles.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [2.81s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello Charles! It's nice to meet you. How can I help you today? If you have any questions or need assistance, feel free to ask.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Initial message: \" Hello Charles! It's nice to meet you. How can I help you today? If you have any questions or need assistance, feel free to ask.\"\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Well, what is my name?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [3.77s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" As an AI language model, I don't have real-time awareness or knowledge of your name. However, I can help you answer questions about names, meanings, or origins. If you want to know more about a specific name or discuss naming conventions, feel free to ask!\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Follow-up message: \" As an AI language model, I don't have real-time awareness or knowledge of your name. However, I can help you answer questions about names, meanings, or origins. If you want to know more about a specific name or discuss naming conventions, feel free to ask!\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM \n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "print(f'Initial message: {llm.predict(\"Hello, my name is Charles.\")}')\n",
    "print(f'Follow-up message: {llm.predict(\"Well, what is my name?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d238-cf65-4356-bbc6-797acef6fed1",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÁúãÁúãÂ¶Ç‰ΩïÂú®LangChain‰∏≠‰∏∫‰∏Ä‰∏™ÂØπËØùÂ∫îÁî®Á®ãÂ∫èÊ∑ªÂä†‰∏Ä‰∏™ËÆ∞ÂøÜÊ®°Âùó„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®ConversationBufferMemoryËÆ∞ÂøÜÊ®°Âùó„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1731ea5-edd8-42c3-9a3b-a4d1ccc02913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial message: \" Hello, Charles! It\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to discuss or do you just want to chat? I\\\\'m here to assist you in any way I can.\\n\"\n",
      "Follow-up message: \" \\\"Charles, your name is Charles.\\\\n\\\"\\n\\nIf you have any other questions or would like to chat about something else, please feel free to ask.\\\\n\\\"\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# [2] Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [3] Memory should work with Chain for effect\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b293b-93c1-4c4c-9b41-e0ecda1d0c31",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394b53-6399-4e64-9a51-a9544a62590e",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊ£ÄÁ¥¢ÊñπÂºèÔºåÂç≥Âü∫‰∫éÂêëÈáèÂ≠òÂÇ®ÁöÑÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ÁúãÁúãÂÆÉÂú®LangChainÁªÑ‰ª∂‰∏≠ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "496c96be-ce08-4cb3-8b2c-6cc0d5b060a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ÊµÅÊµ™Âú∞ÁêÉ.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\document_loaders\\text.py:41\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 41\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xa7 in position 28: illegal multibyte sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# [1] Load content from disk file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÊµÅÊµ™Âú∞ÁêÉ.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# [2] Transform file content into splits for storage and retrieve\u001b[39;00m\n\u001b[0;32m     16\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\document_loaders\\text.py:54\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading ÊµÅÊµ™Âú∞ÁêÉ.txt"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from disk file\n",
    "loader = TextLoader('ÊµÅÊµ™Âú∞ÁêÉ.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# [5] Retriever can be directly accessed from vector store instance\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"ÊµÅÊµ™Âú∞ÁêÉËÆ°Âàí\")\n",
    "\n",
    "# [6] Interate around retrieved documents and print first 100 characters of each\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f'doc #{i}: {doc.page_content[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0779-3941-46ac-955e-944f708a23bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 LangChain: Hands-On ÁªÉ‰π†4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780749e-1242-4815-92ce-ec87d6617ed9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Âú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂÄüÂä©LangChainÊ°ÜÊû∂ÊûÑÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑLLMÂ∫îÁî®Á®ãÂ∫è„ÄÇÊàë‰ª¨Âç≥Â∞ÜÊûÑÂª∫ÁöÑÂ∫îÁî®Á®ãÂ∫èÊòØ‰∏Ä‰∏™ÊñáÊ°£ËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂÖÅËÆ∏ÊÇ®Â∞±ÊñáÊ°£Êñá‰ª∂ÁöÑÂÜÖÂÆπÊèêÂá∫ÈóÆÈ¢ò„ÄÇÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ[Chatbot](https://python.langchain.com/docs/use_cases/chatbots)„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a0149-d6a9-4ce2-bec8-17c549ba5bd4",
   "metadata": {},
   "source": [
    "**step1:**\n",
    "\n",
    "ËÆ©Êàë‰ª¨È¶ñÂÖàÂÆö‰πâË¶Å‰ΩøÁî®ÁöÑLLMÊ®°Âûã„ÄÇ‰∏é‰ª•Ââç‰∏ÄÊ†∑ÔºåÂèØ‰ª•‰ΩøÁî®Êô∫Ë∞±AI„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8eb571-9c5c-4df0-a945-0485da2806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab12d-7617-48ed-95fa-99058b316f5e",
   "metadata": {},
   "source": [
    "**step2:**\n",
    "  \n",
    "ÁÑ∂ÂêéÔºåÂàõÂª∫‰∏Ä‰∏™Áî®‰∫éÂ≠òÂÇ®ÂéÜÂè≤ËÅäÂ§©Ê∂àÊÅØÁöÑËÆ∞ÂøÜÔºåËøô‰ΩøÂæóËÅäÂ§©Êú∫Âô®‰∫∫ËÉΩÂ§üËÆ∞‰ΩèÂÖàÂâçÁöÑÂØπËØù„ÄÇÂú®ËøôÈáåÔºå‰∏çÂÜç‰ΩøÁî®‰πãÂâçÁöÑConversationBufferMemoryÔºåËÄåÊòØÂ∞ùËØïÂè¶‰∏ÄÁßçËÆ∞ÂøÜÔºåÂç≥ConversationSummaryMemory„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db120e36-3347-4cf8-9eeb-7123b0e69f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0727dc7-45d1-41ab-b008-b2f2a3174ff9",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºåConversationSummaryMemoryÊé•Âèó‰∏Ä‰∏™Âêç‰∏∫llmÁöÑÂèÇÊï∞„ÄÇ\n",
    "\n",
    "ÂÆûÈôÖ‰∏äÔºåËøô‰∏™ËÆ∞ÂøÜ‰øùÁïô‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÂéÜÂè≤ÂØπËØù‰ø°ÊÅØÔºåÂç≥ÂéÜÂè≤Ê∂àÊÅØÁöÑÂàóË°®ÂíåÂéÜÂè≤Ê∂àÊÅØÁöÑÁÆÄÁü≠ÊëòË¶Å„ÄÇ\n",
    "\n",
    "‰∏éConversationBufferMemoryÁõ∏ÊØîÔºåÊëòË¶ÅÁöÑ‰ΩøÁî®‰ΩøÊàë‰ª¨‰∏ç‰ºö‰ΩøLLM‰∏ä‰∏ãÊñáÁ™óÂè£Ôºà‰ª§ÁâåÈôêÂà∂ÔºâÂèòÂæóËáÉËÇø„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4d11-4386-4cb2-8460-ed7af7d0f60c",
   "metadata": {},
   "source": [
    "**step3:**\n",
    "\n",
    "‰πãÂêéÔºåËÆ©Êàë‰ª¨ÂÆåÊàêÊ£ÄÁ¥¢Âô®ÈÉ®ÂàÜÔºåÂç≥Âä†ËΩΩÊñáÊ°£„ÄÅÊãÜÂàÜÊñáÊú¨„ÄÅËΩ¨Êç¢‰∏∫ÂµåÂÖ•Âπ∂Â≠òÂÇ®Âú®Êï∞ÊçÆÂ∫ì‰∏≠„ÄÇ\n",
    "  \n",
    "‰∏é‰πãÂâç‰∏ÄÊ†∑ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®FAISSÂêëÈáèÂ≠òÂÇ®„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce06be-b6ea-477c-90f3-825d577168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadÊï∞ÊçÆËµÑÊ∫ê\n",
    "# Write your code here\n",
    "blog_url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa9dbe-8cb9-4bbe-a4e5-d1b542e891ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊãÜÂàÜÊï∞ÊçÆÊàêÂùó\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b684fe-7e26-431c-81a5-4ca01b7eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂêëÈáèÂ§ÑÁêÜÂ≠òÂÖ•ÂêëÈáèÊï∞ÊçÆÂ∫ì\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e21d4-6c61-4e48-a65b-e9647b662255",
   "metadata": {},
   "source": [
    "**step4:**\n",
    "ÊúÄÂêéÔºåËÆ©Êàë‰ª¨Â∞Ü‰∏äËø∞ÁªÑ‰ª∂ÁªÑÂêàÊàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑÈìæ„ÄÇÊàë‰ª¨‰ΩøÁî®ÁöÑÈìæÊòØ`ConversationalRetrievalChain`„ÄÇËØ•ÈìæÁöÑÂ∑•‰ΩúÊñπÂºèÂ¶Ç‰∏ãÔºö\n",
    "\n",
    "1. ‰ΩøÁî®ËÅäÂ§©ÂéÜÂè≤ÂíåÊñ∞ÈóÆÈ¢òÂàõÂª∫‰∏Ä‰∏™‚ÄúÁã¨Á´ãÈóÆÈ¢ò‚Äù„ÄÇ\n",
    "2. Â∞ÜËøô‰∏™Êñ∞ÈóÆÈ¢ò‰º†ÈÄíÁªôÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ËøîÂõûÁõ∏ÂÖ≥ÊñáÊ°£„ÄÇ\n",
    "3. Â∞ÜÊ£ÄÁ¥¢Âà∞ÁöÑÊñáÊ°£‰∏éÊñ∞ÈóÆÈ¢òÔºàÈªòËÆ§Ë°å‰∏∫ÔºâÊàñÂéüÂßãÈóÆÈ¢òÂíåËÅäÂ§©ÂéÜÂè≤‰∏ÄËµ∑‰º†ÈÄíÁªôLLMÔºåÁîüÊàêÊúÄÁªàÁöÑÂìçÂ∫î„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837b92-bc2f-4a5e-aa56-7dbb6445af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e45e2-ab5c-4677-b75f-d9f796b50081",
   "metadata": {},
   "source": [
    "**step5:**\n",
    "  \n",
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÊµãËØï‰∏Ä‰∏ãÊàë‰ª¨ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bb80e-f4c8-4ffe-ac01-4e2b97371748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question One: 'How do agents use Task decomposition?'\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5ec0-54bb-44cc-8f32-b0d8e4f57844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Two: 'What are the various ways to implement memory to support it?'\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a79a5a-670c-425b-b287-a0bd77f12acc",
   "metadata": {},
   "source": [
    "## 3. Âü∫‰∫éLLMÁöÑAgentÔºàÂü∫‰∫éOpenAIÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe2e3-6903-46b8-ab75-99443ce79605",
   "metadata": {},
   "source": [
    "**Agent: Hands-On**\n",
    " \n",
    "AgentsÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÈÄâÊã©‰∏ÄÁ≥ªÂàóË¶ÅÊâßË°åÁöÑÂä®‰Ωú„ÄÇ\n",
    "\n",
    "ËÄåÂú®Chains‰∏≠Ôºå‰∏ÄÁ≥ªÂàóÂä®‰ΩúÊòØÁ°¨ÁºñÁ†ÅÁöÑÔºàÂú®‰ª£Á†Å‰∏≠Ôºâ\n",
    "\n",
    "Âú®Agent‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãË¢´Áî®‰ΩúÊé®ÁêÜÂºïÊìéÔºåÁ°ÆÂÆöË¶ÅÊâßË°åÂì™‰∫õÂä®‰Ωú‰ª•ÂèäÈ°∫Â∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49329a-2433-49c0-8951-a2f8ae61443a",
   "metadata": {},
   "source": [
    "‰∏∫‰∫ÜÊîØÊåÅÊûÑÂª∫Âü∫‰∫éLLMÁöÑAgentÔºâÔºåLangChainÊèê‰æõ‰∫Ü‰ª•‰∏ãÊ®°ÂùóÂåñÁªÑ‰ª∂ÔºåÂç≥\n",
    "\n",
    "* `Tool`ÔºöÂåÖË£Ö‰∫Ü‰∏Ä‰∏™PythonÂáΩÊï∞ÂíåÁõ∏Â∫îÁöÑÊñáÊú¨ÊèèËø∞ÔºåÂÆÉËµã‰∫àAgentË∞ÉÁî®Â§ñÈÉ®Â∑•ÂÖ∑ÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇËÆ°ÁÆóÂô®„ÄÅPythonËß£ÈáäÂô®„ÄÅÊêúÁ¥¢ÂºïÊìéAPI„ÄÇ\n",
    "* `Agent`ÔºöÊâ©Â±ï‰∫ÜÊôÆÈÄöÁöÑLangChain`Chain`Ê®°ÂùóÔºåÂÖ∑Êúâ‰∏ÄÁªÑ`Tool`Ôºå‰ª•ÂèäÁî®‰∫é‰∏≠Èó¥Ê≠•È™§ÁöÑÊèêÁ§∫Ôºà‰æãÂ¶ÇReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚ÄùËøΩË∏™ÔºâÔºå‰ª£ÁêÜÊâßË°åÁöÑËæìÂá∫Ë¶Å‰πàÊòØË¶ÅÈááÂèñÁöÑ‰∏ã‰∏Ä‰∏™Âä®‰ΩúÔºà`AgentAction`ÔºâÔºåË¶Å‰πàÊòØÂèëÈÄÅÁªôÁî®Êà∑ÁöÑÊúÄÁªàÂìçÂ∫îÔºà`AgentFinish`Ôºâ„ÄÇ\n",
    "* `AgentExecutor`ÔºöÊòØAgentÁöÑËøêË°åÊó∂ÔºåÂÆÉÂÆûÈôÖ‰∏äË∞ÉÁî®`Agent`ÔºåÊâßË°åÂÆÉÈÄâÊã©ÁöÑÂä®‰ΩúÔºåÂ∞ÜÂä®‰ΩúÁöÑËæìÂá∫‰º†ÈÄíÂõûAgentÔºåÁÑ∂ÂêéÈáçÂ§çÔºåÁõ¥Âà∞ËææÂà∞`AgentFinish`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2490-2e78-45ac-afaa-52fe2f4f7947",
   "metadata": {},
   "source": [
    "> ‚ùó ÂáÜÂ§áÊÇ®ÁöÑAPIÂØÜÈí•\n",
    ">\n",
    "> Á°Æ‰øùÊÇ®Â∑≤ÁªèÊåâÁÖßÂÖàÂÜ≥Êù°‰ª∂ËÆæÁΩÆ‰∫ÜÂºÄÂèëÁéØÂ¢ÉÔºåÂπ∂Êã•ÊúâË∞ÉÁî®LLMÊúçÂä°ÁöÑÊúâÊïàAPIÂØÜÈí•ÔºåËøôÈáå‰ª•OpenAI‰∏∫‰æã„ÄÇ\n",
    ">\n",
    "> ËØ∑Á°Æ‰øùÊÇ®Â∑≤Áªè‰ªéÁéØÂ¢ÉÂèòÈáè‰∏≠Âä†ËΩΩ‰∫ÜOpenAPIÂØÜÈí•‰ª•‰æõ‰ΩøÁî®ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbdc6f-19e4-4b3b-acd7-0a9724be0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example `.env` config file.\n",
    "![[ -f .env ]] || echo \"OPENAI_API_KEY==<your_zhipu_ai_api_key>\" > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603e0e5e-937d-4b6b-9236-556f2b54bc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56076b-6b0f-460a-a7af-39a34ca3154c",
   "metadata": {},
   "source": [
    "### 3.1 Tool: Python Function + Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7f6d-5f81-4074-a75b-8d3a6979f80f",
   "metadata": {},
   "source": [
    "È¶ñÂÖàÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏ãLangChainÁé∞ÊàêÊèê‰æõÁöÑ‰∏Ä‰∫õÂÜÖÁΩÆTool„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b65cc613-fb87-45e4-90ac-ea16e93d8277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting numexpr\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/89/1b180ac1a8e0ee81f497ff491cfe917173b0fe42b3ad77e4afdbf3d5aecc/numexpr-2.8.7-cp311-cp311-macosx_11_0_arm64.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m583.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/zhiqiang/anaconda3/envs/env1/lib/python3.11/site-packages (from numexpr) (1.26.2)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.8.7\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63423193-4fee-44bc-9ee8-abacea1e954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python_repl', 'requests', 'requests_get', 'requests_post', 'requests_patch', 'requests_put', 'requests_delete', 'terminal', 'sleep', 'wolfram-alpha', 'google-search', 'google-search-results-json', 'searx-search-results-json', 'bing-search', 'metaphor-search', 'ddg-search', 'google-serper', 'google-scholar', 'google-serper-results-json', 'searchapi', 'searchapi-results-json', 'serpapi', 'dalle-image-generator', 'twilio', 'searx-search', 'wikipedia', 'arxiv', 'golden-query', 'pubmed', 'human', 'awslambda', 'sceneXplain', 'graphql', 'openweathermap-api', 'dataforseo-api-search', 'dataforseo-api-search-json', 'eleven_labs_text2speech', 'google_cloud_texttospeech', 'news-api', 'tmdb-api', 'podcast-api', 'memorize', 'llm-math', 'open-meteo-api']\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# [1] Some tools rely on LLM during its execution\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "\n",
    "math_tools = load_tools(['llm-math'], llm=llm)  # [1] Tool for arithmetic calculation\n",
    "meteo_tools = load_tools(['open-meteo-api'], llm=llm)  # [2] Tool for weather info\n",
    "wiki_tools = load_tools(['wikipedia'])  # [3] Tool for searching on Wikipedia\n",
    "\n",
    "# [4] Print total list of builtin tool names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c6b5-b8bb-4217-ab30-7133da36993d",
   "metadata": {},
   "source": [
    "#### `llm_math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c018ad41-7e01-4002-8c0f-3b8df5a84405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Math: 2 + 2 => Answer: 4\n",
      "LLM Math: (4829 + 2930) * 1923 => Answer: 14920557\n",
      "Pure LLM: \n",
      "The sum of 4829 and 2930 is 7759. \n",
      "\n",
      "To find the result after multiplying by 1923, we would multiply 7759 by 1923.\n",
      "\n",
      "7759 multiplied by 1923 equals 14,930,157.\n"
     ]
    }
   ],
   "source": [
    "llm_math = math_tools[0]\n",
    "\n",
    "# [1] Try a simple equation.\n",
    "print(f'LLM Math: 2 + 2 => {llm_math.run(\"What is 2 + 2?\")}')\n",
    "\n",
    "# [2] How about a slightly diffucult one? Recall that pure LLM may fail on this example.\n",
    "print(f'LLM Math: (4829 + 2930) * 1923 => {llm_math.run(\"Sum 4829 and 2930, and then multiply by 1923.\")}')\n",
    "\n",
    "# [3] Pure LLM failed to reach the correct answer.\n",
    "print(f'Pure LLM: \\n{llm.predict(\"Sum 4829 and 2930, and then multiply by 1923.\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ec366-cb1a-4fa4-9547-54d17a3761c8",
   "metadata": {},
   "source": [
    "#### `open-meteo-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f8c7563-6a31-4333-a5b2-9312ab4ef08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Paris is 8.2¬∞C with a windspeed of 14.6 km/h coming from the northwest direction. The weather condition is identified by the WMO code 61.\n"
     ]
    }
   ],
   "source": [
    "meteo = meteo_tools[0]\n",
    "print(meteo.run(\"What's the weather in Paris?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c0e8e80-b1f6-4227-b5f2-3215f980ed18",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÈô§‰∫Ü‰ΩøÁî®LangChainÊèê‰æõÁöÑÂÜÖÁΩÆToolÔºåÊàë‰ª¨ËøòÂèØ‰ª•ÂÆö‰πâËá™Â∑±ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•‰æø‰ΩøÁî®ÁÆÄÂçïÁöÑ‰ª£Á†Å„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fbfecfa-6da0-47b1-9f92-7e04a830b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool  # [1] We use the `tool` decorator to create new `Tool` instance\n",
    "def time(text: str) -> str:\n",
    "    # [2] The docstring (wrapped in \"\"\" \"\"\") are used as tool description\n",
    "    #     (which is sent to LLM when used by agent)\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())  # [3] The actual logic for this `Tool`, i.e, return today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a412d745-de60-4321-86f4-a59a4e7fa1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-11-24'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.run('')  # Note the input is not used in our customed `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504426-27d8-4c3d-a143-cb6a419ae12c",
   "metadata": {},
   "source": [
    "Âè¶‰∏Ä‰∏™Ëá™ÂÆö‰πâÂ∑•ÂÖ∑ÔºåÂÆÉÊé•ÂèóÂ§ö‰∏™ÂèÇÊï∞‰Ωú‰∏∫ËæìÂÖ•Âπ∂ËøîÂõû‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂ≠óÁ¨¶‰∏≤„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fde986d1-1517-41fc-b214-f5f60baf8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str:\n",
    "    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n",
    "    result = requests.post(url, json=body, params=parameters)\n",
    "    return f\"Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9415-cd30-437b-b302-3fa458751a7d",
   "metadata": {},
   "source": [
    "### 3.2  Agent: Chain Equipped with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20e32a-9e08-4709-af24-ab0f22b97c17",
   "metadata": {},
   "source": [
    "LangChainÂ∑≤ÁªèÂÆö‰πâ‰∫Ü‰∏Ä‰∫õÂÜÖÁΩÆÁöÑAgentÁ±ªÂûãÔºåÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•Âú®ÂÖ∂Âü∫Á°Ä‰∏äÊûÑÂª∫Êàë‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe993ae0-9c7e-4be1-b853-24f2cc79c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZERO_SHOT_REACT_DESCRIPTION', 'REACT_DOCSTORE', 'SELF_ASK_WITH_SEARCH', 'CONVERSATIONAL_REACT_DESCRIPTION', 'CHAT_ZERO_SHOT_REACT_DESCRIPTION', 'CHAT_CONVERSATIONAL_REACT_DESCRIPTION', 'STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION', 'OPENAI_FUNCTIONS', 'OPENAI_MULTI_FUNCTIONS']\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.types import AgentType\n",
    "print([item.name for item in AgentType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7ce7e-c657-40f8-80a8-71aae8a9fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÔºåÂç≥ZERO_SHOT_REACT_DESCRIPTIONÔºåÂÆÉÁ±ª‰ºº‰∫éÈõ∂-shot ReActÈ£éÊ†ºÁöÑAgent„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acb4fb12-d468-4ea1-8b4b-65e2f1ada7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224d308-7681-475c-9c5e-f983ff6f1920",
   "metadata": {},
   "source": [
    "ËØ∑Ê≥®ÊÑèÔºåLangChain‰∏≠ÁöÑ`Agent`Êú¨Ë∫´‰∏çËøêË°åÔºåÁõ∏ÂèçÔºåÂÆÉÂÆö‰πâ‰∫ÜÈÄÇÂΩìÁöÑLLM„ÄÅÂ∑•ÂÖ∑ÂíåÊèêÁ§∫ÔºåÂú®`AgentExecutor`‰∏≠ÊâßË°åÊó∂‰ΩøÁî®„ÄÇËÆ©Êàë‰ª¨ÁúãÁúã`ZeroShotAgent`ÊòØÂ¶Ç‰ΩïÊûÑÂª∫ÂÖ∂ÊèêÁ§∫ÁöÑ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9616-15d7-4739-82e4-a8df23af4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bea3d-908f-45f7-8885-1934563f28ae",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºå`{input}` ÂÆö‰πâ‰∫ÜÁî®Êà∑ËæìÂÖ•ÊàñÈóÆÈ¢òÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºå‚ÄúÂì™ÊîØÁêÉÈòüËµ¢Âæó‰∫Ü2022Âπ¥ÁöÑFIFA‰∏ñÁïåÊùØÔºü‚ÄùÔºõ`{agent_scratchpad}` ÊòØ‰ª£ÁêÜÂëàÁé∞ÂÖ∂Ëøõ‰∏ÄÊ≠•ÊâßË°åÁöÑ‰∏≠Èó¥Ê≠•È™§ÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºåReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚Äù‰∏âÂÖÉÁªÑÂ∫èÂàó„ÄÇÊåâËÆæËÆ°ÔºåÂú®LangChain‰∏≠ÔºåÊØè‰∏™`Agent`ÈÉΩÂ∫îËØ•Âú®ÂÖ∂ÊèêÁ§∫Ê®°Êùø‰∏≠ÂÆö‰πâ‰∏Ä‰∏™ÂèòÈáè`{agent_scratchpad}`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34599-40d1-4119-a8e5-63fb2a8e840a",
   "metadata": {},
   "source": [
    "### 3.3 AgentExecutor: Where Agents Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e704dc9-fc43-4e69-a41e-321cc17b6e6e",
   "metadata": {},
   "source": [
    "`AgentExecutor`ÊòØ`Agent`ÔºàÂ∞±ÂÉèÊàë‰ª¨‰∏äÈù¢ÂÆö‰πâÁöÑÈÇ£Ê†∑ÔºâÂÆûÈôÖÊâßË°åÁöÑÂú∞Êñπ„ÄÇÊ†πÊçÆÊàë‰ª¨Â∏åÊúõ‰ª£ÁêÜËøêË°åÁöÑÊñπÂºèÔºåÂèØ‰ª•Êúâ‰∏çÂêåÁ±ªÂûãÁöÑ`AgentExecutor`„ÄÇÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Â∏åÊúõ‰ΩøÁî®LangChainÊèê‰æõÁöÑÈªòËÆ§`AgentExecutor`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2be4-e2de-444d-bf65-244954077f6e",
   "metadata": {},
   "source": [
    "‰ª•‰∏ã‰ª£Á†ÅÁâáÊÆµÊù•Ëá™`AgentExecutor`ÔºåÂ±ïÁ§∫‰∫ÜLangChain‰∏≠ÈÄöÂ∏∏Â¶Ç‰ΩïÊâßË°å`Agent`„ÄÇ\n",
    "```python\n",
    "class AgentExecutor(Chain):\n",
    "    ...\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        ...\n",
    "        # [1] To prevent `Agent`s from running into an infinite loop, `AgentExecutor` use\n",
    "        #     both number of LLM invocations (`iterations`) and used time (`time_elapsed`)\n",
    "        #     to stop execution even if `Agent` do not want to finish\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # [2] We now enter into the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            # [3] Take a single step in the \"Thought/Action/Observation\" loop, \n",
    "            #     return either `AgentAction` plus input or `AgentFinish`\n",
    "            next_step_output = self._take_next_step(...)\n",
    "            if isinstance(next_step_output, AgentFinish):  # [4] Return if LLM decides to finish\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "    \n",
    "            intermediate_steps.extend(next_step_output)  # [5] Store current step, i.e, `AgentAction` plus input\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:  # [6] Check the next `AgentAction` wants to return directly\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # [7] Deal with early stop, can still return something even if stopped in the middle\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e894-a713-425e-8bd6-f05e3da36a55",
   "metadata": {},
   "source": [
    "### 3.4 Put It Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb57e-c577-4954-a7c2-4bf07089347d",
   "metadata": {},
   "source": [
    "Áé∞Âú®ËÆ©Êàë‰ª¨Â∞Ü`Tool`„ÄÅ`Agent`Âíå`AgentExecutor`ÁªìÂêàËµ∑Êù•ÔºåÁúãÁúãLangChain‰ª£ÁêÜÊúâÂì™‰∫õÂäüËÉΩ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8990dd5d-478d-4ebf-87f5-0a4afcb2bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is the weather in Berlin? Raise it to the power of 2.', 'output': 'The weather in Berlin is currently 7.2¬∞C and when raised to the power of 2, it is 51.84.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools, AgentExecutor\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(executor.invoke('What is the weather in Berlin? Raise it to the power of 2.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce705842-df82-4862-abd2-7e44aabfc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
