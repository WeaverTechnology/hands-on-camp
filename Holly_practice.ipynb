{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ebed44-a2d2-4965-9048-7d4caece6d82",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. ÂáÜÂ§áÁéØÂ¢É"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bdcf5-f21e-40f2-8979-548cb1db6ba5",
   "metadata": {},
   "source": [
    "### 1.1 ÂÆâË£Ö‰æùËµñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66b4b-15f8-47fa-a6c0-70795799ad31",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÂÆâË£Ö‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÂ∫ìÔºå‰æãÂ¶Ç langchain Âíå python-dotenv„ÄÇ\n",
    "\n",
    "ÂâçËÄÖ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊûÑÂª∫Âü∫‰∫éLLMÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÊ®°ÂùóÂåñÊ°ÜÊû∂ÔºåËÄåÂêéËÄÖÂú®‰∏∫Âú®Á∫øLLMÊúçÂä°ËÆæÁΩÆAPIÂØÜÈí•ÊñπÈù¢‰∏∫Êàë‰ª¨ËäÇÁúÅ‰∫ÜÊó∂Èó¥ÔºàÊúâÂÖ≥ËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇËßÅ‰∏ã‰∏ÄËäÇÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0608b18-2faf-40f8-beaf-efbd713e4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langchain==0.0.338\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ce/3f/1dafc52526337d1c554227b0e6f16a1aee18e63bf5cd03fd7774297059b2/langchain-0.0.338-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.3/2.0 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.5/2.0 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.8/2.0 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.3/2.0 MB 5.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/2.0 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.0 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (3.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.338)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/e2/528c52001a743a7faa28e6d3095d9f01b472d3efee62d62101403bf1a70a/dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.338)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.338)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/84/9e/208314830d8c523dae4dec41ab5aeeb2d42dc1667bbc3ff8b875244b3012/langsmith-0.0.66-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.8/46.8 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\lenovo\\lib\\site-packages (from langchain==0.0.338) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\lenovo\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.338) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\lenovo\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\lenovo\\lib\\site-packages (from anyio<4.0->langchain==0.0.338) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.4/49.4 kB ? eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.338)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\lenovo\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.338) (2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\lenovo\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.338) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lenovo\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lenovo\\lib\\site-packages (from requests<3,>=2->langchain==0.0.338) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\lenovo\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.338) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\lenovo\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\lenovo\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.338) (1.0.0)\n",
      "Installing collected packages: typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 langchain-0.0.338 langsmith-0.0.66 marshmallow-3.20.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install langchain, the library we will learn during our courses\n",
    "!pip install langchain==0.0.338 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892e98f2-bbf1-4c8f-a755-5a8c64e9a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/44/2f/62ea1c8b593f4e093cc1a7768f0d46112107e790c3e478532329e434f00b/python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install dotenv, auto-load environment variables from `.env` files\n",
    "!pip install python-dotenv==1.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919c726-32d3-4025-9e8f-db00e5ca1c74",
   "metadata": {},
   "source": [
    "Ê≠§Â§ñÔºåËÆ©Êàë‰ª¨ÂÆâË£ÖÁî®‰∫éÂØπÂÜÖÂÆπËøõË°åÊ†áËÆ∞ÂåñÂíåÂ≠òÂÇ®Âú®ÂêëÈáèÊï∞ÊçÆÂ∫ì‰∏äÁöÑÂ∫ìÔºåÂç≥ tiktoken Âíå faiss-cpu„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3e332c-0324-42ca-b5ca-8be73cfea0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken==0.5.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b8/eb/234646d9eefda8a500d0fd88b05bf625a90ed18054124349db26e558276e/tiktoken-0.5.1-cp311-cp311-win_amd64.whl (759 kB)\n",
      "     ---------------------------------------- 0.0/759.8 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 71.7/759.8 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------- -------------------------- 225.3/759.8 kB 2.3 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 368.6/759.8 kB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 634.9/759.8 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 759.8/759.8 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\lenovo\\lib\\site-packages (from tiktoken==0.5.1) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\lenovo\\lib\\site-packages (from tiktoken==0.5.1) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\lenovo\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lenovo\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lenovo\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lenovo\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.7.22)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken, the library used by OpenAI models for tokenizing text strings\n",
    "!pip install tiktoken==0.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55858b86-2ff1-46a7-a473-78295437ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting faiss-cpu==1.7.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/8f/55f8cd863c678a703aa3b97e5f4511cfd2c52987ba05459afdcdec76eeb3/faiss_cpu-1.7.4-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/10.8 MB 1.5 MB/s eta 0:00:08\n",
      "      --------------------------------------- 0.2/10.8 MB 2.4 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/10.8 MB 3.0 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.7/10.8 MB 3.6 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.1/10.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.5/10.8 MB 5.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.9/10.8 MB 5.7 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.4/10.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.7/10.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.2/10.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.6/10.8 MB 6.9 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.1/10.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.5/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.7/10.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.2/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.6/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.9/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.3/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 6.6/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.0/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.4/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 7.8/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.3/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.7/10.8 MB 7.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.2/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.6/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.0/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.4/10.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.8/10.8 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# Install faiss-cpu, a vector database for storing content along with embedding vectors\n",
    "!pip install faiss-cpu==1.7.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2865033a-0683-4b6a-a478-8f6b9d120765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting wikipedia==1.4.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\lenovo\\lib\\site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\lenovo\\lib\\site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\lenovo\\lib\\site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.4)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11707 sha256=96411b3db3e3d31e4738948c5697708f9659e02f938004673cc0a89f41634b19\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\4c\\15\\09\\7fb39e00467370dc55c69e9574c7c6a3ab3085d7c41b3d8ae5\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5f5f4-63d3-441d-8c9a-f7976f33555c",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÂÆâË£Ö‰∏Ä‰∫õÁî®‰∫éËÆøÈóÆÂ§ñÈÉ®ÊúçÂä°ÁöÑÂ∫ìÔºå‰æãÂ¶Ç wikipedia„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a14613b-82a4-40b2-8c82-770d5cb7b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: wikipedia==1.4.0 in d:\\lenovo\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\lenovo\\lib\\site-packages (from wikipedia==1.4.0) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\lenovo\\lib\\site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lenovo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\lenovo\\lib\\site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.4)\n"
     ]
    }
   ],
   "source": [
    "# Install wikipedia, the library for accessing wikipedia service in code\n",
    "!pip install wikipedia==1.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d50-d9ac-44b3-b7c6-25b084474119",
   "metadata": {},
   "source": [
    "ÊúÄÂêéÔºå‰∏∫‰∫ÜÊµãËØïÂÆâË£ÖÂíåAPIÂØÜÈí•ÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨ËøòÂÆâË£ÖÁõ∏Â∫î‰æõÂ∫îÂïÜÁöÑSDKÂ∫ìÔºàÂç≥OpenAIÂíåÊô∫Ë∞±AIÔºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2623724c-492a-4737-be44-0ae014214042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openai==1.3.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/95/22a9a81cebd54e18841da429f05f06ed867648768f7af938ad34f13197fd/openai-1.3.3-py3-none-any.whl (220 kB)\n",
      "     ---------------------------------------- 0.0/220.3 kB ? eta -:--:--\n",
      "     ---------- ---------------------------- 61.4/220.3 kB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------- --------- 163.8/220.3 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 220.3/220.3 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in d:\\lenovo\\lib\\site-packages (from openai==1.3.3) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.3.3)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f4/2c/c90a3adaf0ddb70afe193f5ebfb539612af57cffe677c3126be533df3098/distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.3.3)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/75.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 75.0/75.0 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\lenovo\\lib\\site-packages (from openai==1.3.3) (1.10.8)\n",
      "Requirement already satisfied: tqdm>4 in d:\\lenovo\\lib\\site-packages (from openai==1.3.3) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in d:\\lenovo\\lib\\site-packages (from openai==1.3.3) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\lenovo\\lib\\site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\lenovo\\lib\\site-packages (from anyio<4,>=3.5.0->openai==1.3.3) (1.2.0)\n",
      "Requirement already satisfied: certifi in d:\\lenovo\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.3.3) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.3)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.9/76.9 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.3)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\lenovo\\lib\\site-packages (from tqdm>4->openai==1.3.3) (0.4.6)\n",
      "Installing collected packages: h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.3\n"
     ]
    }
   ],
   "source": [
    "# Install openai, official SDK by OpenAI for invoking GPT models\n",
    "!pip install openai==1.3.3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1338b17d-d953-4742-9fed-e8a2080170b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting zhipuai==1.0.7\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/b5/cb50cd478426b17503154585be799e09506b70944e2fe2402aa029987fa2/zhipuai-1.0.7-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: requests in d:\\lenovo\\lib\\site-packages (from zhipuai==1.0.7) (2.31.0)\n",
      "Requirement already satisfied: PyJWT in d:\\lenovo\\lib\\site-packages (from zhipuai==1.0.7) (2.4.0)\n",
      "Collecting dataclasses (from zhipuai==1.0.7)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting cachetools (from zhipuai==1.0.7)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\lenovo\\lib\\site-packages (from requests->zhipuai==1.0.7) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lenovo\\lib\\site-packages (from requests->zhipuai==1.0.7) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lenovo\\lib\\site-packages (from requests->zhipuai==1.0.7) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lenovo\\lib\\site-packages (from requests->zhipuai==1.0.7) (2023.7.22)\n",
      "Installing collected packages: dataclasses, cachetools, zhipuai\n",
      "Successfully installed cachetools-5.3.2 dataclasses-0.6 zhipuai-1.0.7\n"
     ]
    }
   ],
   "source": [
    "# Install zhipu, official SDK by OpenAI for invoking ChatGLM models\n",
    "!pip install zhipuai==1.0.7 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06972013-340f-427f-9038-12cde927c9a7",
   "metadata": {},
   "source": [
    "### 1.2 ÁéØÂ¢ÉÂèòÈáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6cfdeb-fdbf-4a09-a8fa-0ed98e470606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ZHIPUAI_API_KEY']='7b214aa5ecf1a6ded0877045a4db3a2c.KteBwto1w2UO2COl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017855e-c925-4264-9319-e47ebcbe250b",
   "metadata": {},
   "source": [
    "### 1.3 ÊµãËØïÂáÜÂ§áÊòØÂê¶ÊàêÂäü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af19cdeb-20c5-44aa-85b2-6df9422bf5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ÊàëÁöÑÂêçÂ≠óÊòØÁÆÄ„ÄÇ‰Ω†ÁöÑÂêçÂ≠óÂè´‰ªÄ‰πàÔºü\"\n"
     ]
    }
   ],
   "source": [
    "# Test zhipuai installation\n",
    "import os\n",
    "import zhipuai\n",
    "\n",
    "zhipuai.api_key = os.getenv('ZHIPUAI_API_KEY')  # Set API key from envrionment variable\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "completion = zhipuai.model_api.invoke(\n",
    "    model='chatglm_turbo',\n",
    "    prompt=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0.,\n",
    ")\n",
    "\n",
    "print(completion['data']['choices'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68940f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 200,\n",
       " 'msg': 'Êìç‰ΩúÊàêÂäü',\n",
       " 'data': {'request_id': '8142534961285981110',\n",
       "  'task_id': '8142534961285981110',\n",
       "  'task_status': 'SUCCESS',\n",
       "  'choices': [{'role': 'assistant', 'content': '\" ÊàëÁöÑÂêçÂ≠óÊòØÁÆÄ„ÄÇ‰Ω†ÁöÑÂêçÂ≠óÂè´‰ªÄ‰πàÔºü\"'}],\n",
       "  'usage': {'prompt_tokens': 32, 'completion_tokens': 10, 'total_tokens': 42}},\n",
       " 'success': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0678d-8625-41ce-b335-cb09274c868d",
   "metadata": {},
   "source": [
    "## 2. LangchainÂü∫Á°ÄÁªÉ‰π†ÔºàÂü∫‰∫éÊô∫Ë∞±LLMÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b07742-bfc3-4e59-888f-f5619a9fe247",
   "metadata": {},
   "source": [
    "‰∏éOpenAI‰∏çÂêåÔºåLangChainÂπ∂‰∏çÂéüÁîüÊîØÊåÅÊô∫Ë∞±AIÁöÑÂú®Á∫øLLMÊúçÂä°„ÄÇÁõ∏ÂèçÔºåÊàë‰ª¨ÂèØ‰ª•ÁºñÂÜô‰∏Ä‰∏™ÂåÖË£ÖÁ±ªÊù•Â∞ÜÊô∫Ë∞±AIÁöÑChatGLPÊúçÂä°ÁßªÊ§çÂà∞LangChainÔºåËøôË¶ÅÂΩíÂäü‰∫éLangChainÁöÑÊ®°ÂùóÂåñÊé•Âè£„ÄÇËøôÂ∫îËØ•Á±ª‰ºº‰∫éÊàë‰ª¨‰ΩøÁî®OpenAIÁöÑGPTÊúçÂä°Êó∂ÁöÑÊÑüËßâ„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c752ae-14ff-451b-a30d-cb1190340da2",
   "metadata": {},
   "source": [
    "### 2.1 Ê£ÄÊü•ZhipuAI wrapperÊòØÂê¶Â≠òÂú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4c40040-163c-4df3-a869-15a4fa931043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' ‰∏çÊòØÂÜÖÈÉ®ÊàñÂ§ñÈÉ®ÂëΩ‰ª§Ôºå‰πü‰∏çÊòØÂèØËøêË°åÁöÑÁ®ãÂ∫è\n",
      "ÊàñÊâπÂ§ÑÁêÜÊñá‰ª∂„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "# Check ZhipuAI wrapper existence\n",
    "!ls -la | grep \"zhipuai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e2d6-86c1-4900-8398-fa25a3559aea",
   "metadata": {},
   "source": [
    "### 2.2 ÁÆÄÂçï‰ΩøÁî®‰æãÂ≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18bd3d2f-b446-42a6-9920-7ad218dfc003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ÊàëÂè´ÁÆÄ„ÄÇ‰Ω†Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"You will be provided with a sentence in English, and your task is to translate it into Chinese.\n",
    "\n",
    "My name is Jane. What is yours?\n",
    "\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88ab0-0ba3-4140-a939-ae91f39b4521",
   "metadata": {},
   "source": [
    "#### ÁªÉ‰π†1 - \"ËÆ°ÁÆóÊó∂Èó¥Â§çÊùÇÂ∫¶\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc11a5-6e42-4419-b3f6-ad8d0f2fa105",
   "metadata": {},
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> \n",
    "> ```\n",
    "> You will be provided with Python code, and your task is to calculate its time complexity.\n",
    ">\n",
    "> def foo(n, k):\n",
    ">    accum = 0\n",
    ">    for i in range(n):\n",
    ">        for l in range(k):\n",
    ">            accum += i\n",
    ">    return accum\n",
    "> ```\n",
    "> \n",
    "> ---------------------------\n",
    "> Try to change the Python code for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f1456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d167350b-bca4-4bd0-9a15-0f10e8d6187d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "prompt = \"\"\"You will be provided with Python code, and your task is to calculate its time complexity.\n",
    "\n",
    "def foo(n, k):\n",
    "   accum = 0\n",
    "   for i in range(n):\n",
    "       for l in range(k):\n",
    "           accum += i\n",
    "   return accum\n",
    "\"\"\"\n",
    "response = llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b887f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" To calculate the time complexity of the given code, we need to analyze the number of operations performed in the code.\\\\n\\\\nIn this code, there are two nested loops:\\\\n\\\\n1. The outer loop iterates from 0 to n-1, where n is the input size. So, there are n iterations.\\\\n2. The inner loop iterates from 0 to k-1, where k is the input size. So, there are k iterations.\\\\n\\\\n Inside the inner loop, there is one addition operation performed on each iteration.\\\\n\\\\nÂõ†Ê≠§ÔºåÊÄªÂÖ±Êúâ n * k ‰∏™Ê∑ªÂä†Êìç‰Ωú„ÄÇ\\\\n\\\\nTime complexity = n * k\\\\n\\\\nÂÅáËÆæ n = 1000 Âíå k = 1000ÔºåÂàôÊó∂Èó¥Â§çÊùÇÂ∫¶‰∏∫Ôºö\\\\n\\\\nTime complexity = 1000 * 1000 = 1,000,000\\\\n\\\\nSoÔºåthe time complexity of the given code is O(n * k).\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19a3a-dd46-464b-8612-c103dd0e3427",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†2 - ‚ÄúÂæÆÂçöÊÉÖÊÑüÂàÜÊûê‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d45ed-cc60-4d04-a4c6-6719721e4889",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "> positive, neutral, or negative.\n",
    "> \n",
    "> I loved the new Batman movie!\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the tweet text for analysis and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e4b7ba-2bb0-46cc-9407-7855b101710c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" positive\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here.\n",
    "\n",
    "prompt=\"\"\"You will be provided with a tweet, and your task is to classify its sentiment as \n",
    "positive, neutral, or negative.\n",
    "\n",
    "I loved the new Batman movie!\n",
    "\"\"\"\n",
    "response=llm.predict(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f1a19-cabc-4801-9178-9024748a8a8d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### ÁªÉ‰π†3 - ‚ÄúÊú∫Âú∫‰ª£Âè∑ÊèêÂèñ‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf980e2b-c9c0-4cdf-8b44-062027f8fa8c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> üí™ Practice yourself.\n",
    "> Please finish the code for this task, with the following prompt example:\n",
    ">\n",
    "> ---------------------------\n",
    "> ```\n",
    "> You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "> \n",
    "> I want to fly from Orlando to Boston\n",
    "> ```\n",
    ">\n",
    "> ---------------------------\n",
    "> Try to change the city names and see how LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a10b019-a071-407a-8e4e-0fb13cdbc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "prompt=\"\"\"You will be provided with a text, and your task is to extract the airport codes from it.\n",
    "\n",
    "I want to fly from Orlando to Boston\n",
    "\n",
    "\"\"\"\n",
    "response=llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c5d4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" ORD, BOS\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09091de-7a0a-4338-918c-cca0608a2397",
   "metadata": {},
   "source": [
    "### 2.3 Êé¢Á¥¢LLMÂ±ÄÈôê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6a1b5-e44e-47ff-9584-8d3040106eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Which team won the 1986 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 1st response: {response}')\n",
    "\n",
    "prompt = \"\"\"Which team won the 2022 FIFA World Cup?\"\"\"\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "print(f'- 2nd response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1d69741-cf70-453b-abb8-b865aee1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpt: \" First, let's add 4829 and 2930:\\n\\n4829 + 2930 = 7759\\n\\nNow, let's multiply the sum by 1923:\\n\\n7759 * 1923 = 147,196,467\\n\\nSo, (4829 + 2930) * 1923 = 147,196,467.\"\n",
      "- truth:\n",
      "\n",
      " 14920557\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "prompt = \"\"\"Sum 4829 and 2930, and then multiply by 1923.\"\"\"\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(f'- gpt: {response}')\n",
    "print(f'- truth:\\n\\n {(4829 + 2930) * 1923}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15a3b-39e5-4420-b81e-75a4aed444e0",
   "metadata": {},
   "source": [
    "### 2.4 Êé¢Á¥¢LangchainÊ®°ÂùóÂåñÁªÑ‰ª∂ËÆæËÆ°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72490709-13f7-4fd1-9fa6-6fed0c61e876",
   "metadata": {},
   "source": [
    "üìå ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°Âºè\n",
    "\n",
    "Â¶ÇÊûúÊÇ®ÊòØÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®Âú®LangChain‰∏≠ÊâìÂºÄË∞ÉËØïÂíåËØ¶ÁªÜÊ®°ÂºèÔºåÂú®LLMÂ∫îÁî®Á®ãÂ∫èÊâßË°åËøáÁ®ã‰∏≠ÊòæÁ§∫‰∏≠Èó¥Ê≠•È™§ÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ\n",
    "Êü•ÁúãÊèêÁ§∫Â¶Ç‰ΩïÂ°´ÂÖÖ‰ª•Âèä‰∏≠Èó¥LLMÁîüÊàêÁöÑÂìçÂ∫îÊòØ‰∏™Â•Ω‰∏ªÊÑèÔºàÂú®Ê≠£Â∏∏Ê®°Âºè‰∏ã‰∏çÂ∫îÊâìÂç∞‰ªª‰ΩïËæìÂá∫Ôºâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6857b586-03a6-4700-a1a5-83b3e034a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ee3a0-2cf6-4dd8-9a4e-f1dfb70227db",
   "metadata": {},
   "source": [
    "#### Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afa2ad97-46ce-424a-afb7-a94b7ef674ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"animals\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"animals\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"animals\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: animals\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ZhipuAILLM] [3.60s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Dog, Cat, Horse, Elephant, Penguin\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\\\" Dog, Cat, Horse, Elephant, Penguin\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:CommaSeparatedListOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" Dog\",\n",
      "    \"Cat\",\n",
      "    \"Horse\",\n",
      "    \"Elephant\",\n",
      "    \"Penguin\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    \"\\\" Dog\",\n",
      "    \"Cat\",\n",
      "    \"Horse\",\n",
      "    \"Elephant\",\n",
      "    \"Penguin\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\" Dog', 'Cat', 'Horse', 'Elephant', 'Penguin\"']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Custom output parser, split comma separated strings and return as list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# [2] System message template, declare task requirement as prompt\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# [3] Human message template, here we use Python format string syntax\n",
    "# (https://docs.python.org/3/library/string.html#formatstrings)\n",
    "human_template = '{text}'\n",
    "\n",
    "# [4] We send both messages to LLM for response\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# [5] Build up simple chain with LangChain Expression Language\n",
    "# (https://python.langchain.com/docs/expression_language/)\n",
    "chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()\n",
    "\n",
    "# [6] Call simple chain with human input, i.e., text = \"colors\"\n",
    "chain.invoke({'text': 'animals'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8281d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RunnableSequence.invoke of ChatPromptTemplate(input_variables=['text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])\n",
       "| ZhipuAILLM(client=<class 'zhipuai.model_api.api.ModelAPI'>, model='chatglm_turbo', zhipuai_api_key='7b214aa5ecf1a6ded0877045a4db3a2c.KteBwto1w2UO2COl')\n",
       "| CommaSeparatedListOutputParser()>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd5c10-9981-474c-9ad2-288ede059f10",
   "metadata": {},
   "source": [
    "#### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40377-0d36-4250-9585-41dc0a63ea56",
   "metadata": {},
   "source": [
    "Âú®Êé•‰∏ãÊù•ÁöÑÈÉ®ÂàÜÔºåÊàë‰ª¨Â∞Ü‰∏ìÊ≥®‰∫é‰º†ÁªüÁöÑChainÊé•Âè£„ÄÇÈ¶ñÂÖàÂºÄÂßãÈáçÂÜôÂâç‰∏ÄËäÇ‰∏≠ÁöÑICELÈ£éÊ†ºÈìæ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "652675c6-df3b-426e-b6b6-97efc878af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"colors\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.\\nHuman: colors\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [4.07s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" red, blue, green, yellow, purple\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [4.07s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": [\n",
      "    \"\\\" red\",\n",
      "    \"blue\",\n",
      "    \"green\",\n",
      "    \"yellow\",\n",
      "    \"purple\\\"\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['\" red', 'blue', 'green', 'yellow', 'purple\"']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = '{text}'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', template),\n",
    "    ('human', human_template),\n",
    "])\n",
    "\n",
    "# Equivalent to `chain = chat_prompt | ZhipuAILLM(model='chatglm_turbo') | CommaSeparatedListOutputParser()`\n",
    "chain = LLMChain(\n",
    "    llm=ZhipuAILLM(model='chatglm_turbo'),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "\n",
    "chain.invoke({'text': 'colors'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c4120dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant who generates comma separated lists.\\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\\nONLY return a comma separated list, and nothing more.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d039d6c-ad90-4b0b-bbba-1a71588878d6",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™Êõ¥Â§çÊùÇÁöÑÈìæ„ÄÇÊàë‰ª¨Â∞Ü‰ªãÁªç‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰∏§Èò∂ÊÆµËøûÁª≠ÈìæÔºåÂÖ∂‰∏≠Ôºö\n",
    "\n",
    "1. ‰∏∫‰∏ÄÂÆ∂Âà∂ÈÄ†ÊüêÁßç‰∫ßÂìÅÁöÑÂÖ¨Âè∏ÊèêÂá∫ÂêçÁß∞\n",
    "2. ‰∏∫ÊèêÂá∫ÁöÑÂÖ¨Âè∏ÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑÊèèËø∞ÔºàÂç≥Âè£Âè∑Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca8bcd6b-f5e7-44f0-90bd-192e352c91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Pure Milk\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is the best name to describe a company that makes Pure Milk?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:ZhipuAILLM] [6.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\" Pure Milk Professionals,\\\\\\\" \\\\\\\"Wholesome Dairy,\\\\\\\" \\\\\\\"Clean Milk Company,\\\\\\\" \\\\\\\"Real Milk Solutions,\\\\\\\" or \\\\\\\"Purewhite Dairy.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for its consumers.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] [6.16s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" A company that makes pure milk can be named \\\\\\\" Pure Milk Professionals,\\\\\\\" \\\\\\\"Wholesome Dairy,\\\\\\\" \\\\\\\"Clean Milk Company,\\\\\\\" \\\\\\\"Real Milk Solutions,\\\\\\\" or \\\\\\\"Purewhite Dairy.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for its consumers.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"company_name\": \"\\\" A company that makes pure milk can be named \\\\\\\" Pure Milk Professionals,\\\\\\\" \\\\\\\"Wholesome Dairy,\\\\\\\" \\\\\\\"Clean Milk Company,\\\\\\\" \\\\\\\"Real Milk Solutions,\\\\\\\" or \\\\\\\"Purewhite Dairy.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for its consumers.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a 20 words slogan for the following company:\\\" A company that makes pure milk can be named \\\\\\\" Pure Milk Professionals,\\\\\\\" \\\\\\\"Wholesome Dairy,\\\\\\\" \\\\\\\"Clean Milk Company,\\\\\\\" \\\\\\\"Real Milk Solutions,\\\\\\\" or \\\\\\\"Purewhite Dairy.\\\\\\\" These names emphasize the company's focus on producing high-quality, pure milk for its consumers.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] [3.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Experience the Pure White Gold: Clean, Wholesome Milk for a healthier lifestyle!\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] [3.05s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Experience the Pure White Gold: Clean, Wholesome Milk for a healthier lifestyle!\\\\\\\"\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] [9.21s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\\" \\\\\\\"Experience the Pure White Gold: Clean, Wholesome Milk for a healthier lifestyle!\\\\\\\"\\\"\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" \\\\\"Experience the Pure White Gold: Clean, Wholesome Milk for a healthier lifestyle!\\\\\"\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "product = 'Pure Milk'\n",
    "\n",
    "# [0] The same LLM instance shared by both chains (remember LLM is stateless)\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [1] Build name chain (1st chain)\n",
    "name_template = \"\"\"What is the best name to describe a company that makes {product}?\"\"\"\n",
    "name_prompt = ChatPromptTemplate.from_template(name_template)\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "# [2] Build slogan chain (2nd chain)\n",
    "slogan_template = \"\"\"Write a 20 words slogan for the following company:{company_name}\"\"\"\n",
    "slogan_prompt = ChatPromptTemplate.from_template(slogan_template)\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "# [3] Construct final chain in a sequencial manner\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, slogan_chain])\n",
    "\n",
    "# [4] Call our final chain to propose and write slogan\n",
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cee8a2-5574-4f87-9d45-8f2161edd5b6",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dca0f-aef3-4a36-8edc-d68d55c40893",
   "metadata": {},
   "source": [
    "ÂõûÈ°æ‰∏Ä‰∏ãÊàë‰ª¨ËØ¥ËøáÁöÑLLMÊú¨Ë¥®‰∏äÊòØÊó†Áä∂ÊÄÅÁöÑÔºåÂç≥ÂêéÁª≠Ë∞ÉÁî®Ê∞∏Ëøú‰∏ç‰ºöÂõûÂøÜËµ∑Âú®‰πãÂâçÁöÑË∞ÉÁî®‰∏≠ÊèêÂà∞ÁöÑ‰ø°ÊÅØ„ÄÇËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÊù•ËØ¥ÊòéËøô‰∏™ËØ¥Ê≥ï„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06801d81-a3f2-46e1-a7db-1a33bba1b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Hello, my name is Charles.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [4.52s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello Charles! It's nice to meet you. How can I help you today? If you have any questions or need assistance, feel free to ask.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Initial message: \" Hello Charles! It's nice to meet you. How can I help you today? If you have any questions or need assistance, feel free to ask.\"\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Well, what is my name?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ZhipuAILLM] [4.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" As an AI language model, I don't have real-time awareness or the ability to know your name without you providing it. Please feel free to share your name, and I'll be happy to address you by your preferred name.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Follow-up message: \" As an AI language model, I don't have real-time awareness or the ability to know your name without you providing it. Please feel free to share your name, and I'll be happy to address you by your preferred name.\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "print(f'Initial message: {llm.predict(\"Hello, my name is Charles.\")}')\n",
    "print(f'Follow-up message: {llm.predict(\"Well, what is my name?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d238-cf65-4356-bbc6-797acef6fed1",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÁúãÁúãÂ¶Ç‰ΩïÂú®LangChain‰∏≠‰∏∫‰∏Ä‰∏™ÂØπËØùÂ∫îÁî®Á®ãÂ∫èÊ∑ªÂä†‰∏Ä‰∏™ËÆ∞ÂøÜÊ®°Âùó„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®ConversationBufferMemoryËÆ∞ÂøÜÊ®°Âùó„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1731ea5-edd8-42c3-9a3b-a4d1ccc02913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Hello, my name is Charles.\",\n",
      "  \"chat_history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\n\\n\\nNew human question: Hello, my name is Charles.\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [5.56s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5.56s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\"\n",
      "}\n",
      "Initial message: \" Hello, Charles! It\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\'m here to help!\\n\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Well, what is my name?\",\n",
      "  \"chat_history\": \"Human: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\\n\\nNew human question: Well, what is my name?\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [4.92s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [4.92s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\"\n",
      "}\n",
      "Follow-up message: \" \\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\"\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# [1] Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# [2] Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "llm = ZhipuAILLM(model='chatglm_turbo', temperature=0.7)\n",
    "\n",
    "# [3] Memory should work with Chain for effect\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a96ae83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Hello, my name is Charles.\",\n",
      "  \"chat_history\": \"Human: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\\nHuman: Well, what is my name?\\nAI: \\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\\nHuman: Well, what is my name?\\nAI: \\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\\n\\nNew human question: Hello, my name is Charles.\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [5.99s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Hello, Charles! It's a pleasure to meet you again. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I'm here to help!\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5.99s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Hello, Charles! It's a pleasure to meet you again. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I'm here to help!\\\"\"\n",
      "}\n",
      "Initial message: \" Hello, Charles! It's a pleasure to meet you again. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I'm here to help!\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Well, what is my name?\",\n",
      "  \"chat_history\": \"Human: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\\nHuman: Well, what is my name?\\nAI: \\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It's a pleasure to meet you again. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I'm here to help!\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a nice chatbot having a conversation with a human.\\n\\nPrevious conversation:\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It\\\\\\\\'s a pleasure to meet you. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I\\\\\\\\'m here to help!\\\\n\\\"\\nHuman: Well, what is my name?\\nAI: \\\" \\\\\\\"Charles, your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in my previous response. Is there anything else you would like to chat about or do you need assistance with something else? Let me know, I'm here to help!\\\\\\\"\\\"\\nHuman: Hello, my name is Charles.\\nAI: \\\" Hello, Charles! It's a pleasure to meet you again. How can I help you today? Is there a specific topic you would like to chat about or do you just want to hang out and chat about anything? Let me know if you need any assistance or if you have any questions. I'm here to help!\\\"\\n\\nNew human question: Well, what is my name?\\nResponse:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [5.74s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" \\\\\\\"Hello, Charles. Your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in our previous conversations. If you have any other questions or if there's anything else I can help you with, please don't hesitate to ask. I'm here for you.\\\\\\\"\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [5.74s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" \\\\\\\"Hello, Charles. Your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in our previous conversations. If you have any other questions or if there's anything else I can help you with, please don't hesitate to ask. I'm here for you.\\\\\\\"\\\"\"\n",
      "}\n",
      "Follow-up message: \" \\\"Hello, Charles. Your name is Charles. You previously introduced yourself as Charles, and I acknowledged your name in our previous conversations. If you have any other questions or if there's anything else I can help you with, please don't hesitate to ask. I'm here for you.\\\"\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial message: {chain.invoke(\"Hello, my name is Charles.\")[\"text\"]}')\n",
    "print(f'Follow-up message: {chain.invoke(\"Well, what is my name?\")[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b293b-93c1-4c4c-9b41-e0ecda1d0c31",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394b53-6399-4e64-9a51-a9544a62590e",
   "metadata": {},
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊ£ÄÁ¥¢ÊñπÂºèÔºåÂç≥Âü∫‰∫éÂêëÈáèÂ≠òÂÇ®ÁöÑÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ÁúãÁúãÂÆÉÂú®LangChainÁªÑ‰ª∂‰∏≠ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "496c96be-ce08-4cb3-8b2c-6cc0d5b060a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ÊµÅÊµ™Âú∞ÁêÉ.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\document_loaders\\text.py:41\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 41\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xa7 in position 28: illegal multibyte sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# [1] Load content from disk file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÊµÅÊµ™Âú∞ÁêÉ.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# [2] Transform file content into splits for storage and retrieve\u001b[39;00m\n\u001b[0;32m     17\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\document_loaders\\text.py:54\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading ÊµÅÊµ™Âú∞ÁêÉ.txt"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# [1] Load content from disk file\n",
    "\n",
    "loader = TextLoader('ÊµÅÊµ™Âú∞ÁêÉ.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# [2] Transform file content into splits for storage and retrieve\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# [3] Here we invoke embedding functions provided by OpenAI services, which maps text\n",
    "#     string of any size into a fixed size embedding vector, where similar text are\n",
    "#     mapped into vectors of short distance\n",
    "# [4] We use FAISS as our vector store backend to save content along with embedding vectors\n",
    "embeddings = ZhipuAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# [5] Retriever can be directly accessed from vector store instance\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"ÊµÅÊµ™Âú∞ÁêÉËÆ°Âàí\")\n",
    "\n",
    "# [6] Interate around retrieved documents and print first 100 characters of each\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f'doc #{i}: {doc.page_content[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0779-3941-46ac-955e-944f708a23bc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.5 LangChain: Hands-On ÁªÉ‰π†4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780749e-1242-4815-92ce-ec87d6617ed9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Âú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂÄüÂä©LangChainÊ°ÜÊû∂ÊûÑÂª∫‰∏Ä‰∏™ÁÆÄÂçïÁöÑLLMÂ∫îÁî®Á®ãÂ∫è„ÄÇÊàë‰ª¨Âç≥Â∞ÜÊûÑÂª∫ÁöÑÂ∫îÁî®Á®ãÂ∫èÊòØ‰∏Ä‰∏™ÊñáÊ°£ËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂÖÅËÆ∏ÊÇ®Â∞±ÊñáÊ°£Êñá‰ª∂ÁöÑÂÜÖÂÆπÊèêÂá∫ÈóÆÈ¢ò„ÄÇÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ[Chatbot](https://python.langchain.com/docs/use_cases/chatbots)„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a0149-d6a9-4ce2-bec8-17c549ba5bd4",
   "metadata": {},
   "source": [
    "**step1:**\n",
    "\n",
    "ËÆ©Êàë‰ª¨È¶ñÂÖàÂÆö‰πâË¶Å‰ΩøÁî®ÁöÑLLMÊ®°Âûã„ÄÇ‰∏é‰ª•Ââç‰∏ÄÊ†∑ÔºåÂèØ‰ª•‰ΩøÁî®Êô∫Ë∞±AI„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b8eb571-9c5c-4df0-a945-0485da2806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "llm=ZhipuAILLM(model='chatglm_turbo',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab12d-7617-48ed-95fa-99058b316f5e",
   "metadata": {},
   "source": [
    "**step2:**\n",
    "  \n",
    "ÁÑ∂ÂêéÔºåÂàõÂª∫‰∏Ä‰∏™Áî®‰∫éÂ≠òÂÇ®ÂéÜÂè≤ËÅäÂ§©Ê∂àÊÅØÁöÑËÆ∞ÂøÜÔºåËøô‰ΩøÂæóËÅäÂ§©Êú∫Âô®‰∫∫ËÉΩÂ§üËÆ∞‰ΩèÂÖàÂâçÁöÑÂØπËØù„ÄÇÂú®ËøôÈáåÔºå‰∏çÂÜç‰ΩøÁî®‰πãÂâçÁöÑConversationBufferMemoryÔºåËÄåÊòØÂ∞ùËØïÂè¶‰∏ÄÁßçËÆ∞ÂøÜÔºåÂç≥ConversationSummaryMemory„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db120e36-3347-4cf8-9eeb-7123b0e69f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory,ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "memory=ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_message=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0727dc7-45d1-41ab-b008-b2f2a3174ff9",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºåConversationSummaryMemoryÊé•Âèó‰∏Ä‰∏™Âêç‰∏∫llmÁöÑÂèÇÊï∞„ÄÇ\n",
    "\n",
    "ÂÆûÈôÖ‰∏äÔºåËøô‰∏™ËÆ∞ÂøÜ‰øùÁïô‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑÂéÜÂè≤ÂØπËØù‰ø°ÊÅØÔºåÂç≥ÂéÜÂè≤Ê∂àÊÅØÁöÑÂàóË°®ÂíåÂéÜÂè≤Ê∂àÊÅØÁöÑÁÆÄÁü≠ÊëòË¶Å„ÄÇ\n",
    "\n",
    "‰∏éConversationBufferMemoryÁõ∏ÊØîÔºåÊëòË¶ÅÁöÑ‰ΩøÁî®‰ΩøÊàë‰ª¨‰∏ç‰ºö‰ΩøLLM‰∏ä‰∏ãÊñáÁ™óÂè£Ôºà‰ª§ÁâåÈôêÂà∂ÔºâÂèòÂæóËáÉËÇø„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4d11-4386-4cb2-8460-ed7af7d0f60c",
   "metadata": {},
   "source": [
    "**step3:**\n",
    "\n",
    "‰πãÂêéÔºåËÆ©Êàë‰ª¨ÂÆåÊàêÊ£ÄÁ¥¢Âô®ÈÉ®ÂàÜÔºåÂç≥Âä†ËΩΩÊñáÊ°£„ÄÅÊãÜÂàÜÊñáÊú¨„ÄÅËΩ¨Êç¢‰∏∫ÂµåÂÖ•Âπ∂Â≠òÂÇ®Âú®Êï∞ÊçÆÂ∫ì‰∏≠„ÄÇ\n",
    "  \n",
    "‰∏é‰πãÂâç‰∏ÄÊ†∑ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®FAISSÂêëÈáèÂ≠òÂÇ®„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fce06be-b6ea-477c-90f3-825d577168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadÊï∞ÊçÆËµÑÊ∫ê\n",
    "# Write your code here\n",
    "blog_url = 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(blog_url)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9dfa9dbe-8cb9-4bbe-a4e5-d1b542e891ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊãÜÂàÜÊï∞ÊçÆÊàêÂùó\n",
    "# Write your code here\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits= text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "92b684fe-7e26-431c-81a5-4ca01b7eec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂêëÈáèÂ§ÑÁêÜÂ≠òÂÖ•ÂêëÈáèÊï∞ÊçÆÂ∫ì\n",
    "# Write your code here\n",
    "from langchain.vectorstores import FAISS\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=ZhipuAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e21d4-6c61-4e48-a65b-e9647b662255",
   "metadata": {},
   "source": [
    "**step4:**\n",
    "ÊúÄÂêéÔºåËÆ©Êàë‰ª¨Â∞Ü‰∏äËø∞ÁªÑ‰ª∂ÁªÑÂêàÊàê‰∏Ä‰∏™Âçï‰∏ÄÁöÑÈìæ„ÄÇÊàë‰ª¨‰ΩøÁî®ÁöÑÈìæÊòØ`ConversationalRetrievalChain`„ÄÇËØ•ÈìæÁöÑÂ∑•‰ΩúÊñπÂºèÂ¶Ç‰∏ãÔºö\n",
    "\n",
    "1. ‰ΩøÁî®ËÅäÂ§©ÂéÜÂè≤ÂíåÊñ∞ÈóÆÈ¢òÂàõÂª∫‰∏Ä‰∏™‚ÄúÁã¨Á´ãÈóÆÈ¢ò‚Äù„ÄÇ\n",
    "2. Â∞ÜËøô‰∏™Êñ∞ÈóÆÈ¢ò‰º†ÈÄíÁªôÊ£ÄÁ¥¢Âô®ÔºåÂπ∂ËøîÂõûÁõ∏ÂÖ≥ÊñáÊ°£„ÄÇ\n",
    "3. Â∞ÜÊ£ÄÁ¥¢Âà∞ÁöÑÊñáÊ°£‰∏éÊñ∞ÈóÆÈ¢òÔºàÈªòËÆ§Ë°å‰∏∫ÔºâÊàñÂéüÂßãÈóÆÈ¢òÂíåËÅäÂ§©ÂéÜÂè≤‰∏ÄËµ∑‰º†ÈÄíÁªôLLMÔºåÁîüÊàêÊúÄÁªàÁöÑÂìçÂ∫î„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "33837b92-bc2f-4a5e-aa56-7dbb6445af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa=ConversationalRetrievalChain.from_llm(llm,retriever=retriever,memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e45e2-ab5c-4677-b75f-d9f796b50081",
   "metadata": {},
   "source": [
    "**step5:**\n",
    "  \n",
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÊµãËØï‰∏Ä‰∏ãÊàë‰ª¨ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "295bb80e-f4c8-4ffe-ac01-4e2b97371748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How do agents use Task decomposition?\",\n",
      "  \"chat_history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"How do agents use Task decomposition?\",\n",
      "  \"context\": \"Task decomposition can be done (1) by LLM with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for writing a novel, or (3) with human inputs.\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nTask decomposition can be done (1) by LLM with simple prompting like \\\"Steps for XYZ.\\\\n1.\\\", \\\"What are the subgoals for achieving XYZ?\\\", (2) by using task-specific instructions; e.g. \\\"Write a story outline.\\\" for writing a novel, or (3) with human inputs.\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nQuestion: How do agents use Task decomposition?\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ZhipuAILLM] [20.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [20.53s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 3:chain:StuffDocumentsChain] [20.53s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"\\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [21.76s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": \"\\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: How do agents use Task decomposition?\\nAI: \\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: How do agents use Task decomposition?\\nAI: \\\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\\\" or \\\\\\\"What are the subgoals for achieving XYZ?\\\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\\\"Write a story outline.\\\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent's planning process, enabling them to tackle complex tasks and improve their performance over time.\\\"\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ZhipuAILLM] [8.66s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent's planning process.\\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [8.66s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent's planning process.\\\"\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How do agents use Task decomposition?',\n",
       " 'chat_history': '',\n",
       " 'answer': '\" Agents use task decomposition to break down large tasks into smaller, manageable subgoals. This enables them to efficiently handle complex tasks. Task decomposition can be done in several ways:\\\\n\\\\n1. By LLM with simple prompting: The agent can be prompted to list the steps for a particular task, such as \\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\" or \\\\\"What are the subgoals for achieving XYZ?\\\\\".\\\\n\\\\n2. By using task-specific instructions: For example, if the task is writing a novel, the agent can be instructed to \\\\\"Write a story outline.\\\\\"\\\\n\\\\n3. With human inputs: The agent can leverage human input to understand the task and break it down into smaller subgoals.\\\\n\\\\nTask decomposition allows agents to plan and manage their actions more effectively. By breaking down tasks into smaller subgoals, agents can better understand the overall task and develop a strategy for completing it. This process is crucial for enabling agents to handle complex tasks efficiently and effectively.\\\\n\\\\nAdditionally, agents can use reflection and refinement to improve their performance. By critiquing and reflecting on their past actions, agents can learn from their mistakes and refine their strategies for future steps. This process helps to improve the quality of the final results, as agents are able to adapt and adjust their approaches based on past experience. Overall, task decomposition and reflection are essential components of an agent\\'s planning process, enabling them to tackle complex tasks and improve their performance over time.\"'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question One: 'How do agents use Task decomposition?'\n",
    "# Write your code here.\n",
    "qa('How do agents use Task decomposition?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5de5ec0-54bb-44cc-8f32-b0d8e4f57844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the various ways to implement memory to support it?\",\n",
      "  \"chat_history\": \"\\\" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent's planning process.\\\"\"\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [0ms] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('Unsupported chat history format: <class \\\\'str\\\\'>. Full chat history: \\\" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent\\\\'s planning process.\\\" ')\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported chat history format: <class 'str'>. Full chat history: \" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent's planning process.\" ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Question Two: 'What are the various ways to implement memory to support it?'\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Write your code here.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m qa(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat are the various ways to implement memory to support it?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:135\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    133\u001b[0m question \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    134\u001b[0m get_chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chat_history \u001b[38;5;129;01mor\u001b[39;00m _get_chat_history\n\u001b[1;32m--> 135\u001b[0m chat_history_str \u001b[38;5;241m=\u001b[39m get_chat_history(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_history_str:\n\u001b[0;32m    138\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n",
      "File \u001b[1;32mD:\\LENOVO\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:47\u001b[0m, in \u001b[0;36m_get_chat_history\u001b[1;34m(chat_history)\u001b[0m\n\u001b[0;32m     45\u001b[0m         buffer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([human, ai])\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported chat history format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dialogue_turn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Full chat history: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_history\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m         )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buffer\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported chat history format: <class 'str'>. Full chat history: \" The human asks about how agents use task decomposition to handle complex tasks. The AI explains that task decomposition involves breaking down large tasks into smaller, manageable subgoals, which can be done through various methods such as LLM prompting, task-specific instructions, and human input. This process enables agents to plan and manage their actions more effectively, allowing them to tackle complex tasks efficiently and effectively. Additionally, agents can use reflection and refinement to improve their performance by learning from past mistakes and adjusting their strategies for future steps. Overall, task decomposition and reflection are essential components of an agent's planning process.\" "
     ]
    }
   ],
   "source": [
    "# Question Two: 'What are the various ways to implement memory to support it?'\n",
    "# Write your code here.\n",
    "qa('What are the various ways to implement memory to support it?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a79a5a-670c-425b-b287-a0bd77f12acc",
   "metadata": {},
   "source": [
    "## 3. Âü∫‰∫éLLMÁöÑAgentÔºàÂü∫‰∫éOpenAIÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe2e3-6903-46b8-ab75-99443ce79605",
   "metadata": {},
   "source": [
    "**Agent: Hands-On**\n",
    " \n",
    "AgentsÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØ‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÈÄâÊã©‰∏ÄÁ≥ªÂàóË¶ÅÊâßË°åÁöÑÂä®‰Ωú„ÄÇ\n",
    "\n",
    "ËÄåÂú®Chains‰∏≠Ôºå‰∏ÄÁ≥ªÂàóÂä®‰ΩúÊòØÁ°¨ÁºñÁ†ÅÁöÑÔºàÂú®‰ª£Á†Å‰∏≠Ôºâ\n",
    "\n",
    "Âú®Agent‰∏≠ÔºåËØ≠Ë®ÄÊ®°ÂûãË¢´Áî®‰ΩúÊé®ÁêÜÂºïÊìéÔºåÁ°ÆÂÆöË¶ÅÊâßË°åÂì™‰∫õÂä®‰Ωú‰ª•ÂèäÈ°∫Â∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49329a-2433-49c0-8951-a2f8ae61443a",
   "metadata": {},
   "source": [
    "‰∏∫‰∫ÜÊîØÊåÅÊûÑÂª∫Âü∫‰∫éLLMÁöÑAgentÔºâÔºåLangChainÊèê‰æõ‰∫Ü‰ª•‰∏ãÊ®°ÂùóÂåñÁªÑ‰ª∂ÔºåÂç≥\n",
    "\n",
    "* `Tool`ÔºöÂåÖË£Ö‰∫Ü‰∏Ä‰∏™PythonÂáΩÊï∞ÂíåÁõ∏Â∫îÁöÑÊñáÊú¨ÊèèËø∞ÔºåÂÆÉËµã‰∫àAgentË∞ÉÁî®Â§ñÈÉ®Â∑•ÂÖ∑ÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇËÆ°ÁÆóÂô®„ÄÅPythonËß£ÈáäÂô®„ÄÅÊêúÁ¥¢ÂºïÊìéAPI„ÄÇ\n",
    "* `Agent`ÔºöÊâ©Â±ï‰∫ÜÊôÆÈÄöÁöÑLangChain`Chain`Ê®°ÂùóÔºåÂÖ∑Êúâ‰∏ÄÁªÑ`Tool`Ôºå‰ª•ÂèäÁî®‰∫é‰∏≠Èó¥Ê≠•È™§ÁöÑÊèêÁ§∫Ôºà‰æãÂ¶ÇReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚ÄùËøΩË∏™ÔºâÔºå‰ª£ÁêÜÊâßË°åÁöÑËæìÂá∫Ë¶Å‰πàÊòØË¶ÅÈááÂèñÁöÑ‰∏ã‰∏Ä‰∏™Âä®‰ΩúÔºà`AgentAction`ÔºâÔºåË¶Å‰πàÊòØÂèëÈÄÅÁªôÁî®Êà∑ÁöÑÊúÄÁªàÂìçÂ∫îÔºà`AgentFinish`Ôºâ„ÄÇ\n",
    "* `AgentExecutor`ÔºöÊòØAgentÁöÑËøêË°åÊó∂ÔºåÂÆÉÂÆûÈôÖ‰∏äË∞ÉÁî®`Agent`ÔºåÊâßË°åÂÆÉÈÄâÊã©ÁöÑÂä®‰ΩúÔºåÂ∞ÜÂä®‰ΩúÁöÑËæìÂá∫‰º†ÈÄíÂõûAgentÔºåÁÑ∂ÂêéÈáçÂ§çÔºåÁõ¥Âà∞ËææÂà∞`AgentFinish`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c2490-2e78-45ac-afaa-52fe2f4f7947",
   "metadata": {},
   "source": [
    "> ‚ùó ÂáÜÂ§áÊÇ®ÁöÑAPIÂØÜÈí•\n",
    ">\n",
    "> Á°Æ‰øùÊÇ®Â∑≤ÁªèÊåâÁÖßÂÖàÂÜ≥Êù°‰ª∂ËÆæÁΩÆ‰∫ÜÂºÄÂèëÁéØÂ¢ÉÔºåÂπ∂Êã•ÊúâË∞ÉÁî®LLMÊúçÂä°ÁöÑÊúâÊïàAPIÂØÜÈí•ÔºåËøôÈáå‰ª•OpenAI‰∏∫‰æã„ÄÇ\n",
    ">\n",
    "> ËØ∑Á°Æ‰øùÊÇ®Â∑≤Áªè‰ªéÁéØÂ¢ÉÂèòÈáè‰∏≠Âä†ËΩΩ‰∫ÜOpenAPIÂØÜÈí•‰ª•‰æõ‰ΩøÁî®ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e0e5e-937d-4b6b-9236-556f2b54bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='replace_with_your_open_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56076b-6b0f-460a-a7af-39a34ca3154c",
   "metadata": {},
   "source": [
    "### 3.1 Tool: Python Function + Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7f6d-5f81-4074-a75b-8d3a6979f80f",
   "metadata": {},
   "source": [
    "È¶ñÂÖàÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏ãLangChainÁé∞ÊàêÊèê‰æõÁöÑ‰∏Ä‰∫õÂÜÖÁΩÆTool„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cc613-fb87-45e4-90ac-ea16e93d8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423193-4fee-44bc-9ee8-abacea1e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# [1] Some tools rely on LLM during its execution\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents.load_tools import get_all_tool_names\n",
    "\n",
    "math_tools = load_tools(['llm-math'], llm=llm)  # [1] Tool for arithmetic calculation\n",
    "meteo_tools = load_tools(['open-meteo-api'], llm=llm)  # [2] Tool for weather info\n",
    "wiki_tools = load_tools(['wikipedia'])  # [3] Tool for searching on Wikipedia\n",
    "\n",
    "# [4] Print total list of builtin tool names\n",
    "print(get_all_tool_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c6b5-b8bb-4217-ab30-7133da36993d",
   "metadata": {},
   "source": [
    "#### `llm_math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ad41-7e01-4002-8c0f-3b8df5a84405",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math = math_tools[0]\n",
    "\n",
    "# [1] Try a simple equation.\n",
    "print(f'LLM Math: 2 + 2 => {llm_math.run(\"What is 2 + 2?\")}')\n",
    "\n",
    "# [2] How about a slightly diffucult one? Recall that pure LLM may fail on this example.\n",
    "print(f'LLM Math: (4829 + 2930) * 1923 => {llm_math.run(\"Sum 4829 and 2930, and then multiply by 1923.\")}')\n",
    "\n",
    "# [3] Pure LLM failed to reach the correct answer.\n",
    "print(f'Pure LLM: \\n{llm.predict(\"Sum 4829 and 2930, and then multiply by 1923.\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ec366-cb1a-4fa4-9547-54d17a3761c8",
   "metadata": {},
   "source": [
    "#### `open-meteo-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c7563-6a31-4333-a5b2-9312ab4ef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo = meteo_tools[0]\n",
    "print(meteo.run(\"What's the weather in Paris?\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c0e8e80-b1f6-4227-b5f2-3215f980ed18",
   "metadata": {},
   "source": [
    "ÁÑ∂ÂêéÔºåÈô§‰∫Ü‰ΩøÁî®LangChainÊèê‰æõÁöÑÂÜÖÁΩÆToolÔºåÊàë‰ª¨ËøòÂèØ‰ª•ÂÆö‰πâËá™Â∑±ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•‰æø‰ΩøÁî®ÁÆÄÂçïÁöÑ‰ª£Á†Å„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfecfa-6da0-47b1-9f92-7e04a830b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool  # [1] We use the `tool` decorator to create new `Tool` instance\n",
    "def time(text: str) -> str:\n",
    "    # [2] The docstring (wrapped in \"\"\" \"\"\") are used as tool description\n",
    "    #     (which is sent to LLM when used by agent)\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())  # [3] The actual logic for this `Tool`, i.e, return today's date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412d745-de60-4321-86f4-a59a4e7fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.run('')  # Note the input is not used in our customed `Tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0504426-27d8-4c3d-a143-cb6a419ae12c",
   "metadata": {},
   "source": [
    "Âè¶‰∏Ä‰∏™Ëá™ÂÆö‰πâÂ∑•ÂÖ∑ÔºåÂÆÉÊé•ÂèóÂ§ö‰∏™ÂèÇÊï∞‰Ωú‰∏∫ËæìÂÖ•Âπ∂ËøîÂõû‰∏Ä‰∏™Âçï‰∏ÄÁöÑÂ≠óÁ¨¶‰∏≤„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde986d1-1517-41fc-b214-f5f60baf8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str:\n",
    "    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n",
    "    result = requests.post(url, json=body, params=parameters)\n",
    "    return f\"Status: {result.status_code} - {result.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9415-cd30-437b-b302-3fa458751a7d",
   "metadata": {},
   "source": [
    "### 3.2  Agent: Chain Equipped with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20e32a-9e08-4709-af24-ab0f22b97c17",
   "metadata": {},
   "source": [
    "LangChainÂ∑≤ÁªèÂÆö‰πâ‰∫Ü‰∏Ä‰∫õÂÜÖÁΩÆÁöÑAgentÁ±ªÂûãÔºåÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•Âú®ÂÖ∂Âü∫Á°Ä‰∏äÊûÑÂª∫Êàë‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe993ae0-9c7e-4be1-b853-24f2cc79c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.types import AgentType\n",
    "print([item.name for item in AgentType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7ce7e-c657-40f8-80a8-71aae8a9fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏™‰æãÂ≠êÔºåÂç≥ZERO_SHOT_REACT_DESCRIPTIONÔºåÂÆÉÁ±ª‰ºº‰∫éÈõ∂-shot ReActÈ£éÊ†ºÁöÑAgent„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4fb12-d468-4ea1-8b4b-65e2f1ada7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224d308-7681-475c-9c5e-f983ff6f1920",
   "metadata": {},
   "source": [
    "ËØ∑Ê≥®ÊÑèÔºåLangChain‰∏≠ÁöÑ`Agent`Êú¨Ë∫´‰∏çËøêË°åÔºåÁõ∏ÂèçÔºåÂÆÉÂÆö‰πâ‰∫ÜÈÄÇÂΩìÁöÑLLM„ÄÅÂ∑•ÂÖ∑ÂíåÊèêÁ§∫ÔºåÂú®`AgentExecutor`‰∏≠ÊâßË°åÊó∂‰ΩøÁî®„ÄÇËÆ©Êàë‰ª¨ÁúãÁúã`ZeroShotAgent`ÊòØÂ¶Ç‰ΩïÊûÑÂª∫ÂÖ∂ÊèêÁ§∫ÁöÑ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d9616-15d7-4739-82e4-a8df23af4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bea3d-908f-45f7-8885-1934563f28ae",
   "metadata": {},
   "source": [
    "Ê≥®ÊÑèÔºå`{input}` ÂÆö‰πâ‰∫ÜÁî®Êà∑ËæìÂÖ•ÊàñÈóÆÈ¢òÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºå‚ÄúÂì™ÊîØÁêÉÈòüËµ¢Âæó‰∫Ü2022Âπ¥ÁöÑFIFA‰∏ñÁïåÊùØÔºü‚ÄùÔºõ`{agent_scratchpad}` ÊòØ‰ª£ÁêÜÂëàÁé∞ÂÖ∂Ëøõ‰∏ÄÊ≠•ÊâßË°åÁöÑ‰∏≠Èó¥Ê≠•È™§ÁöÑ‰ΩçÁΩÆÔºå‰æãÂ¶ÇÔºåReAct‰ª£ÁêÜÁöÑ‚ÄúÊÄùËÄÉ/Âä®‰Ωú/ËßÇÂØü‚Äù‰∏âÂÖÉÁªÑÂ∫èÂàó„ÄÇÊåâËÆæËÆ°ÔºåÂú®LangChain‰∏≠ÔºåÊØè‰∏™`Agent`ÈÉΩÂ∫îËØ•Âú®ÂÖ∂ÊèêÁ§∫Ê®°Êùø‰∏≠ÂÆö‰πâ‰∏Ä‰∏™ÂèòÈáè`{agent_scratchpad}`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34599-40d1-4119-a8e5-63fb2a8e840a",
   "metadata": {},
   "source": [
    "### 3.3 AgentExecutor: Where Agents Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e704dc9-fc43-4e69-a41e-321cc17b6e6e",
   "metadata": {},
   "source": [
    "`AgentExecutor`ÊòØ`Agent`ÔºàÂ∞±ÂÉèÊàë‰ª¨‰∏äÈù¢ÂÆö‰πâÁöÑÈÇ£Ê†∑ÔºâÂÆûÈôÖÊâßË°åÁöÑÂú∞Êñπ„ÄÇÊ†πÊçÆÊàë‰ª¨Â∏åÊúõ‰ª£ÁêÜËøêË°åÁöÑÊñπÂºèÔºåÂèØ‰ª•Êúâ‰∏çÂêåÁ±ªÂûãÁöÑ`AgentExecutor`„ÄÇÂ§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Â∏åÊúõ‰ΩøÁî®LangChainÊèê‰æõÁöÑÈªòËÆ§`AgentExecutor`„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f2be4-e2de-444d-bf65-244954077f6e",
   "metadata": {},
   "source": [
    "‰ª•‰∏ã‰ª£Á†ÅÁâáÊÆµÊù•Ëá™`AgentExecutor`ÔºåÂ±ïÁ§∫‰∫ÜLangChain‰∏≠ÈÄöÂ∏∏Â¶Ç‰ΩïÊâßË°å`Agent`„ÄÇ\n",
    "```python\n",
    "class AgentExecutor(Chain):\n",
    "    ...\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, str],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run text through and get agent response.\"\"\"\n",
    "        ...\n",
    "        # [1] To prevent `Agent`s from running into an infinite loop, `AgentExecutor` use\n",
    "        #     both number of LLM invocations (`iterations`) and used time (`time_elapsed`)\n",
    "        #     to stop execution even if `Agent` do not want to finish\n",
    "        iterations = 0\n",
    "        time_elapsed = 0.0\n",
    "        start_time = time.time()\n",
    "        # [2] We now enter into the agent loop (until it returns something).\n",
    "        while self._should_continue(iterations, time_elapsed):\n",
    "            # [3] Take a single step in the \"Thought/Action/Observation\" loop, \n",
    "            #     return either `AgentAction` plus input or `AgentFinish`\n",
    "            next_step_output = self._take_next_step(...)\n",
    "            if isinstance(next_step_output, AgentFinish):  # [4] Return if LLM decides to finish\n",
    "                return self._return(\n",
    "                    next_step_output, intermediate_steps, run_manager=run_manager\n",
    "                )\n",
    "    \n",
    "            intermediate_steps.extend(next_step_output)  # [5] Store current step, i.e, `AgentAction` plus input\n",
    "            if len(next_step_output) == 1:\n",
    "                next_step_action = next_step_output[0]\n",
    "                # See if tool should return directly\n",
    "                tool_return = self._get_tool_return(next_step_action)\n",
    "                if tool_return is not None:  # [6] Check the next `AgentAction` wants to return directly\n",
    "                    return self._return(\n",
    "                        tool_return, intermediate_steps, run_manager=run_manager\n",
    "                    )\n",
    "            iterations += 1\n",
    "            time_elapsed = time.time() - start_time\n",
    "        # [7] Deal with early stop, can still return something even if stopped in the middle\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps, run_manager=run_manager)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603e894-a713-425e-8bd6-f05e3da36a55",
   "metadata": {},
   "source": [
    "### 3.4 Put It Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fb57e-c577-4954-a7c2-4bf07089347d",
   "metadata": {},
   "source": [
    "Áé∞Âú®ËÆ©Êàë‰ª¨Â∞Ü`Tool`„ÄÅ`Agent`Âíå`AgentExecutor`ÁªìÂêàËµ∑Êù•ÔºåÁúãÁúãLangChain‰ª£ÁêÜÊúâÂì™‰∫õÂäüËÉΩ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990dd5d-478d-4ebf-87f5-0a4afcb2bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, AgentExecutor\n",
    "from langchain.agents.mrkl.base import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "tools = load_tools(['llm-math', 'open-meteo-api'], llm=llm)\n",
    "\n",
    "agent = ZeroShotAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(executor.invoke('What is the weather in Berlin? Raise it to the power of 2.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce705842-df82-4862-abd2-7e44aabfc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
